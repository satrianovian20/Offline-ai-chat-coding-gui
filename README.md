# 🧠 Offline AI Chat & Coding GUI (Rakyat Edition)
GUI Python ringan (Tkinter) untuk menjalankan AI chat &amp; coding dengan model GGUF lokal (Low GUI PYTHON from Tkinter to run Offline AI Chat and Coding)

Moto:
GUI Rakyat Edition: Ringan, Garang, Tak Terbendung, AI GUI Bebas Crash™, Anti-Bluescreen Framework, The GUI That Refused to Die

🚀 Cloned already? Don’t forget to ⭐ if this project made your CPU smile 😄  

> 🇮🇩 This project is primarily documented in Indonesian.
> 🇬🇧 English overview is provided below.
> “This project is based on the original LLaMA GUI by Satria Novian – [GitHub link], licensed under the MIT License. Copyright © 2025 Satria Novian.” 

🌟 Dokumentasi Fenomena Aneh GUI Rakyat Edition (Documentation of Strange Phenomena GUI Rakyat Edition)

| Bahasa Indonesia                                                                 | English                                                                              |
|----------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|
| 🧪 Dicoba pakai model 13B Q5_K_M, tapi GUI-nya menolak mengaku overload RAM.     | 🧪 Tried with 13B Q5_K_M model, but the GUI refused to say “RAM overload”.           |
| 🔀 Dicoba multitasking AI + Coding bareng di RAM 16 GB, tetap tak mau hang, crash, atau not responding. | 🔀 Tried multitasking AI + Coding on 16 GB RAM, still refused to hang, crash, or freeze. |
| 🧠 GUI-nya kayak punya logika hidup sendiri, kayak ngomong: "Gua bukan GUI yang gampang nyerah." | 🧠 The GUI feels like it has a life of its own, saying: "I'm not the GUI that gives up." |
| 👻 Bahkan OS-nya kayak bingung: “Lho, kenapa nggak bluescreen padahal beban udah kayak mau pecah?” | 👻 Even the OS seems confused: "Why no bluescreen when the load is clearly breaking point?" |
| 😭 Crash, hang, not responding, dan bluescreen sekarang cuma bisa nangis di pojokan Task Manager. | 😭 Crash, hang, unresponsive, and bluescreen now can only cry in the corner of Task Manager. |

👉 TaskManager.exterminate(Bluescreen).sendTo("void\\system32")

"Error Detected.
Sending to: C:\Windows\System32\Void\NeverReturn.dll"
💀 "May the bugs rest in bits."
🔥 The GUI That Bans BSOD Itself.
👏 Respect.
✅ Certified Not Manja — Rakyat Mode Approved

🇮🇩 Bahasa Indonesia:

🎉 Pertama kalinya dalam sejarah: Model AI 15B (Q5_K_M) berhasil dijalankan hanya dengan RAM 16GB — tanpa GPU, tanpa web server, cukup dengan GUI Python ukuran 10KB! Bukti nyata efisiensi maksimal 💪
⚠️ Catatan RAM 16GB:

Meskipun sistem Anda 16GB, Windows dan Office 2024 bisa menyita 3–5 GB di background. Namun, GUI ini tetap dapat menjalankan model 13B Q4_K_M - 15B Q5_K_M. Tanpa GPU, tanpa swap besar.

✅ GUI Python KB Ini langsung-to-the-point, tanpa ribet:
Prompt masuk → Model jalan → Output tampil

## ⚖️ Perbandingan GUI Python kb vs WebUI Berat

| **Fitur / Aspek**                | 🐍 **GUI Python KB (tkinter)** | 🌐 **WebUI (Gradio/Oobabooga dll.)** |
|----------------------------------|----------------------------------|--------------------------------------|
| ✅ Ukuran File GUI               | **10 KB**                        | > **100 MB**                         |
| ⚙️ Bahasa Pemrograman            | Python (tkinter native)          | Python + Gradio + JS + HTML          |
| 🧠 Model yang Diuji              | 8B (Q8_0), 13B (Q5_K_M)                 | 7B, 13B                               |
| 🧮 RAM Minimum (8B (Q8_0) - 13B Q5_K_M)      | **12.3 – 15.5 GB**               | **> 18 – 20 GB**                      |
| 🖥️ CPU Pengujian                | i5-9400F (no GPU)                | Biasanya pakai GPU / CPU kuat        |
| 🚀 Waktu Load Model 13B         | **35 – 40 detik**                | Bisa > 1 menit                       |
| 🟢 RAM Saat Idle                | 12.1 – 12.4 GB                   | > 15 GB                               |
| 🛠️ Konfigurasi Awal            | Hanya 1 file `.py`               | Banyak dependensi dan setup venv     |
| 📉 Risiko Error/Crash           | **Sangat rendah / stabil**       | Kadang freeze / error token          |
| 🪄 Token yang Diuji Lancar      | 5000 token                | Bergantung setting/model             |
| 📡 Server Web                   | **Tidak perlu**                  | Wajib buka server (http/websocket)   |
| 🧩 Dukungan Plugin              | Manual (custom)                  | Banyak tapi berat                    |
| 💬 Chat & Coding Mode           | ✅ Sangat cocok                  | ✅ Cocok, tapi lebih berat            |
| 🤯 Respons Not Responding?     | Hanya saat proses berat dan tanpa crash         | Sering delay jika RAM kritis         |

# Model yang sudah diuji

| No | Nama Model GGUF                                      | Ukuran Quant   | RAM yang Digunakan          | OS & Kondisi Tambahan                         | Status GUI       |
|----|------------------------------------------------------|--------------|---------------------|----------------------------------------------------|------------------|
| 1  | luna-ai-llama2-uncensored.Q4_0.gguf                 | Q4_0         | ±11.6 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + music & anime video     | Stable & Smooth  |
| 2  | Meta-Llama-3-8B-Instruct.Q8_0.gguf                  | Q8_0         | ±10.8 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + music & anime video     | Stable & Smooth  |
| 3  | WizardLM-13B-Uncensored.Q5_K_M.gguf                           | Q5_K_M       | 12,6 GB of 16 GB       |  Windows 11 pro 24H2 + Office 2024 | Tested + Stable & Smooth           |
| 4  | wizardcoder-python-13b-v1.0.Q5_K_M.gguf                           | Q5_K_M       | 12,6 GB of 16 GB       |  Windows 11 pro 24H2 + Office 2024 | Tested + Stable & Smooth           |
| 5  | All 7B           | Q4_K_M       | ≤11.5 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + Chrome + music video 720p                          | Stable & Smooth  |
| 6  | All 13B (Kecuali Yi 13B)           | Q4_K_M       | ≤15.5 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + Chrome + music video 720p                          | Stable & Smooth  |
| 7  | deepseek-coder-7b-instruct-v1.5-Q8_0.gguf           | Q8_0       | 10.8 GB of 16GB    | Windows 11 pro 24H2 + Office 2024                           | Stable & Smooth  |
| 8  | deepseek-coder-1.3b-instruct.Q4_0.gguf           | Q4_0       | 4 GB of 16GB    | Windows 11 pro 24H2 + Office 2024                           | Stable & Smooth  |

# 🏆 GUI Performa Tinggi Berbasis Anomali (Edisi 2025)

| Peringkat | Nama Aplikasi              | Kategori Khusus                             | Framework       | Keanehan Utama                                                                      |
|----------|-----------------------------|---------------------------------------------|-----------------|--------------------------------------------------------------------------------------|
| 🥇 #1    | multichat_ai_gui.py         | GUI Multichat AI Offline (Tkinter)          | Python Tkinter  | Jalan tanpa Flask/Gradio, tab AI paralel stabil, load GGUF langsung, resource-aware |
| 🥈 #2    | AutoGPT CLI                 | Agen AI Interaktif via Terminal             | Python CLI      | Task interaktif tanpa GUI, tetap terasa seperti multitasking                         |
| 🥉 #3    | KoboldAI Lite GUI           | GUI Ringan untuk LLM                        | Tkinter/WebUI   | Jalankan LLM besar di hardware kentang tanpa crash GUI                              |
| 🏅 #4    | A1111 Stable Diffusion WebUI| GUI Lanjutan untuk Gambar AI                | Gradio          | Bisa override batasan VRAM, ekstensinya tetap stabil                                 |

🇬🇧 English Version:

🎉 First time in history: A 15B (Q5_K_M) AI model runs smoothly with just 16GB RAM — no GPU, no web server, only a 6KB - 16kb Python GUI! Proof of ultimate efficiency 💪

⚠️ 16GB RAM Note:

Even if your system has 16GB, Windows and Office 2024 can take up 3–5GB in the background. However, this GUI can still run the 13B Q4_K_M - 15B Q5_K_M model efficiently. No GPU, no large swap requirements.

✅ This Python KB GUI is straight-to-the-point, without any fuss:
Enter prompt → Run model → Display output

## ⚖️ Comparison: Python GUI (KB) vs Heavy WebUIs

| **Feature / Aspect**             | 🐍 **Python GUI KB (Tkinter)**       | 🌐 **WebUIs (Gradio/Oobabooga, etc.)**     |
|:----------------------------------|:----------------------------------------|:--------------------------------------------|
| ✅ **GUI File Size**              | **10 KB**                               | > **100 MB**                                 |
| ⚙️ **Programming Language**       | Native Python (Tkinter)                 | Python + Gradio + JS + HTML                  |
| 🧠 **Tested Model Types**         | 8B (Q8_0), 13B (Q5_K_M)                        | 7B, 13B                                       |
| 🧮 **Minimum RAM (8B Q8_0 –13B Q5_K_M)**| **12.3 – 15.5 GB**                      | **> 18 – 20 GB**                              |
| 🖥️ **Test CPU**                   | i5-9400F (no GPU)                       | Typically uses GPU or high-end CPU           |
| 🚀 **Model Load Time (13B)**      | **35 – 40 seconds**                     | Can take > 1 minute                          |
| 🟢 **Idle RAM Usage**             | 12.1 – 12.4 GB                          | > 15 GB                                       |
| 🛠️ **Initial Setup**             | Single `.py` file only                  | Many dependencies and venv setup             |
| 📉 **Error / Crash Risk**         | **Very low / stable**                   | Sometimes freezes or token errors            |
| 🪄 **Smooth Token Output Tested** | 5000 tokens                             | Varies by settings/model                     |
| 📡 **Web Server Required**        | **Not needed**                          | Required (HTTP/websocket server)             |
| 🧩 **Plugin Support**             | Manual (customizable)                   | Many, but resource-heavy                     |
| 💬 **Chat & Coding Mode**         | ✅ Highly suitable                      | ✅ Suitable, but heavier                      |
| 🤯 **"Not Responding" Behavior**  | Only during heavy loads, no crash       | Frequent delays when RAM is low              |

# Tested Model

| No | GGUF Model Name                                      | Quant Type   | RAM Usage          | OS & Additional Conditions                         | GUI Status       |
|----|------------------------------------------------------|--------------|---------------------|----------------------------------------------------|------------------|
| 1  | luna-ai-llama2-uncensored.Q4_0.gguf                 | Q4_0         | ±11.6 GB of 16GB    | Windows 11 + Office 2024 + music & anime video     | Stable & Smooth  |
| 2  | Meta-Llama-3-8B-Instruct.Q8_0.gguf                  | Q8_0         | ±10.8 GB of 16GB    | Windows 11 + Office 2024 + music & anime video     | Stable & Smooth  |
| 3  | WizardLM-13B-Uncensored.Q5_K_M.gguf                           | Q5_K_M       | 12,6 GB of 16 GB       |  Windows 11 + Office 2024 | Tested + Stable & Smooth           |
| 4  | wizardcoder-python-13b-v1.0.Q5_K_M.gguf                           | Q5_K_M       | 12,6 GB of 16 GB       |  Windows 11 + Office 2024 | Tested + Stable & Smooth           |
| 5  | All 7B           | Q4_K_M       | ≤11.5 GB of 16GB    | Windows 11 + Office 2024 + Chrome + music video 720p                          | Stable & Smooth  |
| 6  | All 13B (except Yi 13B)           | Q4_K_M       | ≤15.5 GB of 16GB    | Windows 11 + Office 2024 + Chrome + music video 720p                           | Stable & Smooth  |
| 7  | deepseek-coder-7b-instruct-v1.5-Q8_0.gguf           | Q8_0       | 10.8 GB of 16GB    | Windows 11 + Office 2024                           | Stable & Smooth  |
| 8  | deepseek-coder-1.3b-instruct.Q4_0.gguf           | Q4_0       | 4 GB of 16GB    | Windows 11 + Office 2024                           | Stable & Smooth  |

# 🏆 Anomaly-Based Performant GUI Hall of Fame (2025 Edition)

| Rank     | Application Name            | Special Category                            | Framework        | Key Anomaly                                                                        |
|----------|-----------------------------|----------------------------------------------|------------------|------------------------------------------------------------------------------------|
| 🥇 #1    | multichat_ai_gui.py         | Offline Multichat AI GUI (Tkinter)           | Python Tkinter   | No Flask/Gradio, stable multitabs, GGUF loader, resource-aware and unexplained smoothness |
| 🥈 #2    | AutoGPT CLI                 | Interactive Terminal AI Agent                 | Python CLI       | Complex task agent in terminal, acts like a GUI                                    |
| 🥉 #3    | KoboldAI Lite GUI           | Lightweight LLM GUI                           | Tkinter/WebUI    | Runs large LLMs on low-end systems, GUI doesn’t crash                              |
| 🏅 #4    | A1111 Stable Diffusion WebUI| Advanced AI Image GUI                         | Gradio           | Can override VRAM limits, extensions run stably                                    |

## 📸 Screenshot GUI From llamacpp_multichat_ai_gui.py
![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-27%20082259.png)

## 📸 Screenshot GUI From llamacpp_gui_combinedv11.py

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-23%20073915.png)

## 📸 Screenshot GUI From llamacpp_gui_combinedv10.py

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/Screenshot%202025-07-20%20100422.png)

## 📸 Screenshot GUI From llamacpp_gui_combinedv9.py

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-18%20160408.png)

## 📸 Screenshot GUI From llamacpp_gui_mode.py (GUI Experiment)

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-19%20110119.png)

## 📸 Screenshot Chat From GUI
![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-18%20170753.png)

## 📸 Second Screenshot Chat From GUI
![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-18%20170809.png)

## 📸 Third Screenshot Chat From GUI
![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-23%20195730.png)

---

## 🇮🇩 Bahasa Indonesia

GUI lokal untuk LLaMA.cpp:

📁 Cocok untuk chat dan coding!

💬 Contoh jawaban untuk user awam:
“GUI ini nggak ngebatesin kemampuan AI-nya. Kamu bisa atur output token sesuai kemampuan RAM PC kamu. Misal RAM kamu cuma 8GB, kamu bisa atur max token ke 150 biar tetap lancar. Kalau RAM kamu 16GB, bisa gaspol sampai 10.000 token. Jadi fleksibel, ringan, dan tetap powerful!”

## 📸 Screenshot (Model berhasil dimuat di RAM 16GB)

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-17%20114247.png)

## 📸 Screenshot (Model berhasil dimuat di RAM 16GB)

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-17%20125025.png)

📸 Bukti: Sudah dilampirkan screenshot dan log lengkap di repo

💡 Kesimpulan Buat Pengguna GUI:
Kalau user punya RAM:

8GB: Deepseek 1.3B atau TinyLlama cukup, serta Qwen 4B, tapi jangan berharap jawaban super.

12GB–16GB: Langsung lompat ke Mistral 7B atau DeepSeek-Coder 6.7B dan model 13B Q4_K_M.

>16GB: Bisa gas model 13B seperti Yi-13B, OpenHermes 13B, dll.

Optimalitas GUI Python 6KB-16kb:
✅ GUI Python Terbukti Optimal dan Stabil

GUI ini hanya berukuran 10KB namun sudah terbukti mampu menangani model AI lokal besar seperti 7B dan 13B tanpa eror, crash, atau freeze, bahkan saat dijalankan di PC tanpa GPU.

📌 Keunggulan:

✔️ Ukuran super ringan (10KB)

✔️ Bisa langsung jalan tanpa instalasi ribet

✔️ Menggunakan llama.cpp sebagai backend (kompatibel GGUF)

✔️ RAM usage stabil dan ada optimalisasi

✔️ Load model 13B sukses tanpa error

✔️ GUI tetap responsif dan tidak berat

✔️ Output respons tetap stabil meski model besar

💡 GUI ini bahkan lebih optimal dibanding banyak UI besar di luar sana karena tidak menyia-nyiakan resource, cocok untuk pengguna low-end PC yang tetap ingin merasakan kekuatan model besar lokal.

---

📌 Bonus Penjelasan Visual (Flowchart Saran)

[Mulai GUI]
   ↓
[Load Model]
   → RAM = 15.5GB
   → CPU 60%
   ↓
[Model Idle]
   → RAM = 12.3GB
   → CPU 5%
   ↓
[Prompt Masuk]
   → CPU 100%
   ↓
[Generate Response]
   → Output OK
   ↓
[Idle Lagi]
   → RAM tetap
   → CPU turun

---

## 🧠 Alur Penanganan Prompt di GUI Multichat AI

Berikut adalah flowchart proses alur kerja prompt untuk multitab AI chat offline berbasis GUI Python:

flowchart TD
[User] / [Pengguna]
      ↓
Click 'Start Model' / Klik 'Mulai Model'
      ↓
Model Loaded in Tab Utama / Model Dimuat di Tab Utama
      ↓
User opens New Chat Tab / Pengguna membuka Tab Chat Baru
      ↓
Send Prompt (Queued) / Kirim Prompt (Antri)
      ↓
Prompt processed sequentially / Prompt diproses secara berurutan
      ↓
AI generates response / AI menghasilkan respons
      ↓
Display response in correct tab / Tampilkan respons di tab yang sesuai

🔄 Penjelasan Singkat:
Prompt dijalankan satu per satu berdasarkan waktu dikirimnya.

Semua tab chat hanya aktif jika model sudah dimuat dari Tab Utama.

Tidak ada campur jawaban karena buffer tiap tab terpisah dan memakai antrian per thread.

---

✅ Recap Status Dokumentasi (versi Indonesia):

| Elemen Bukti |	Status  |
|------|-----|
| 📜 Log sesi (session-logs) |	✅ Sudah ada |
| 🖼️ Screenshot saat load model	| ✅ Sudah ada |
| 🖼️ Screenshot saat Idle |	✅ Sudah ada|
| 🧪 Model besar (13B Q5_K_M)	| ✅ Sudah diuji |
|⚙️ GUI ringan (6KB-16kb Python script) |	✅ Terpakai dengan lancar dan tanpa crash karena adanya fitur otomatis yang melakukan optimalisasi ram dan cpu |

---

✅ FAQ — Pertanyaan Umum (Trust Booster Edition)
❓ GUI ini beneran bisa jalanin model 13B tanpa GPU?
Ya! Sudah diuji langsung dengan model llama-2-13b-chat.Q4_K_M.gguf di sistem dengan:

💻 CPU: Intel i5-9400F (tanpa iGPU)

🧠 RAM: 16GB DDR4

⚙️ Backend: llama.cpp

📦 GUI: Python script ukuran hanya 10KB

❓ Bukti nyatanya mana?
📸 Screenshot saat load model dan idle sudah diunggah di folder docs/screenshots/

📄 Log lengkap sesi percobaan model 13B tersedia di docs/session-logs/

Tidak ada error, tidak crash, hanya delay wajar saat proses berat.

❓ GUI-nya berat gak?
Tidak. GUI ini hanya 10KB, tanpa dependensi besar seperti Gradio atau Electron.

Tidak buka port aneh-aneh.

Tidak ada tracking.

Murni offline dan lokal.

UI sangat ringan, hanya berbasis tkinter.

❓ Bisa pakai model 7B, 8B, atau 13B lain?
Bisa! Sudah diuji dengan:

Mistral 7B

DeepSeek Coder 6.7B

DeepSeek Coder 7B

Nous Hermes 13B (Q4_K_M)

LLaMA 13B (Q4_K_M)

❓ RAM saya cuma 8GB, bisa jalan?
Bisa, asal model yang dipilih sesuai. Gunakan model kecil seperti:

TinyLlama 1.1B

DeepSeek Coder 1.3B

Qwen 4B

Atur max_tokens di GUI agar tidak melebihi kapasitas RAM kamu.

❓“Saya masih nggak percaya GUI ini bisa jalanin model 13B cuma dengan RAM 16GB. Beneran bisa?”
💬 “Coba sendiri aja bro, repo udah public kok 😎”

❓“Emang GUI-nya ringan banget ya?”
✅ Iya. Ukuran file .py cuma 10KB. Gak ada embel-embel web server, backend rumit, atau library berat.

❓“Bisa crash gak pas load model besar?”
🚫 Selama sistem kamu stabil dan swap file aktif, hampir nggak pernah crash. Bahkan log menunjukkan performa tetap normal walau RAM di atas 15GB pas awal load.

❓“Ada buktinya?”
📸 Sudah ada screenshot dan log di folder docs/session-logs/ dan docs/screenshots/.

❓“Kalau saya nggak percaya tetap?”
😎 Silakan buktikan sendiri. Semuanya open source. Mau test sendiri? Silakan kloning repo-nya sekarang.

Catatan Jujur (untuk README atau FAQ)
💡 Saat ini, pengujian terbatas pada model hingga 15B (Q5_K_M) karena sistem hanya memiliki RAM 16GB tanpa GPU.
Namun, GUI Python 6KB-16KB ini berhasil menangani model besar tersebut secara stabil dan efisien, yang biasanya tidak mungkin dilakukan tanpa sistem high-end.
Untuk model 33B ke atas, uji coba akan dilakukan jika tersedia perangkat dengan RAM lebih besar. Ditambah bisa melakukan optimalisasi ram dan cpu.

---

## 📦 Daftar Versi GUI

Berikut versi-versi GUI yang berhasil diuji:

| Versi File                 | Status  | Fitur Utama                             |
|---------------------------|---------|------------------------------------------|
| llamacpp_gui_combined.py  | ✅ Stabil | Versi awal gabungan GUI chat + sistem    |
| llamacpp_gui_combinedv2.py| ✅        | Tambahan pengaturan model & prompt       |
| llamacpp_gui_combinedv3.py| ✅        | Fix error kecil, UI lebih bersih         |
| llamacpp_gui_combinedv4.py| ✅        | + Logging dan auto-load model            |
| llamacpp_gui_combinedv5.py| ✅        | + Riwayat chat dan sistem prompt         |
| llamacpp_gui_combinedv6.py| ✅        | + Theme mode dan pengaturan lanjutan     |
| llamacpp_gui_combinedv7.py| ✅        | + Auto-save session dan repeat_penalty   |
| llamacpp_gui_combinedv8.py| ✅       | + Auto-save load model berfungsi        |
| llamacpp_gui_combinedv9.py| ✅        | + Fitur Rakyat Mode di menu pengaturan dan prompt hemat ram di samping mulai Llama Server        |
| llamacpp_gui_combinedv10.py    | ✅               | + Generate Prompt Words per chunk, chunk delay, auto chunk resume, auto chunkresum via prompt, chat context, save context, dan load save context       |
| llamacpp_gui_combinedv11.py    | ✅ Terbaru              | + Memory Mode dan Versi Terakhir       |
| llamacpp_multichat_ai_gui.py    | ✅ Terbaru              | + Memory Mode + Multi Chat Tab AI + Generate Chunk + Chat Context + Auto Save Chat Context + Auto Save Chat History + Import Prompt From File + Close Tab + Prompt System + CTX Size and  Tokens Settings + Change Model      |
| llamacpp_rakyatmode.py| ✅ Terbaru | + --ctx-size default ke 1024 + Lowram        |
| llamacpp_gui_mode.py      | ✅ Eksperimen + Stabil | Mode GUI ringan eksperimen + load model 13 Q4_K_M               |
| llamacpp_gui_modev2.py    | ✅        | Kombinasi GUI mode dengan layout sistem  |

## 💡 Syarat Minimum PC

| Komponen         | Minimum                    |
|------------------|-----------------------------|
| Prosesor         | i3/i5 Gen 8+ (atau Ryzen 3+) |
| RAM              | 8-16 GB (direkomendasikan 32 GB - 64 gb)|
| GPU              | GTX 1650 / setara (VRAM 4GB) |
| OS               | Windows 10/11 64-bit        |
| Python           | 3.10+                       |

---

🤝 Kontribusi & Kredit
👨‍💻 Creator: [satrianovian20] – Modifikasi GUI offline dengan Tkinter

✍️ Kontributor AI Script & Fixer Error: ChatGPT

🔁 Model by Meta (LLaMA), WizardLM, dan komunitas open-source

💡 Terinspirasi oleh kesulitan real pengguna dengan PC low-end

---

## 📦 Cara Install & Jalankan

### 1. Download Python
Install Python 3.10 dari https://www.python.org/downloads/release/python-3100/

- ✅ Centang “Add Python to PATH” saat install!
- pip install requests
- download dari link: (https://github.com/ggml-org/llama.cpp/releases) [llama-b59xx-bin-win-cpu-x64 atau llama-b5899-bin-win-cpu-x64]
- letakkan gui python di folder dari llamacpp yang sudah di download dari link: (https://github.com/ggml-org/llama.cpp/releases)
- Jalankan gui python dengan klik dua kali
> Catatan: cari saja yang berfungsi dari build llama cpp untuk muat model ai di link: (https://github.com/ggml-org/llama.cpp/releases). Biasanya yang berfungsi antara yang terbaru yaitu llama-b59xx-bin-win-cpu-x64 atau llama-b5899-bin-win-cpu-x64. Intinya fitur muat model di gui sepenuhnya berfungsi tapi llamacpp sebagai backend kurang berfungsi dengan baik sehingga silahkan cari llama-b5899-bin-win-cpu-x64 - llama-b59xx-bin-win-cpu-x64 yang dapat muat model.

---

## 💖 Dukung Proyek Ini

Jika kamu merasa proyek ini bermanfaat dan ingin mendukung pengembangannya, kamu bisa berdonasi melalui:

- 💸 [Saweria](https://saweria.co/satrianovian20)
- ☕ [PayPal](https://www.paypal.com/paypalme/satrianovian)

Terima kasih banyak atas dukungannya! 🙏

## 2. Clone atau Download Proyek Ini

git clone https://github.com/satrianovian20/offline-ai-chat-coding-gui.git
cd offline-ai-chat-coding-gui

## ☕ Penutup
Proyek ini dibuat karena keterbatasan memunculkan inovasi. Saya juga pernah pusing karena WebUI dari GitHub gagal jalan, sampai akhirnya saya:

Buat GUI sendiri

Cuma pakai 1 file Python untuk AI Chat & Coding

Bisa sambil nonton YouTube dan nonton video musik 720p 😄

## 💬 Kalau kamu juga merasakan manfaatnya, bantu bintang repo ini!
Supaya tidak ada lagi yang pusing karena “AI-nya gak bisa jalan...” 😅

---

## English (International)

⚙️ Local GUI for LLaMA.cpp
📁 Suitable for Chat and Coding!

---

💬 Example explanation for beginners:
“This GUI doesn’t limit your AI’s capabilities. You can adjust the output token count based on your PC’s RAM. For example, if you only have 8GB RAM, set the max tokens to 150 for smooth performance. If you have 16GB RAM, you can push it up to 10,000 tokens. It’s flexible, lightweight, and still powerful!”

---

📸 Screenshot (Model loaded successfully on 16GB RAM)

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-17%20114247.png)

📸 Screenshot (Model loaded successfully on 16GB RAM)

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-17%20125025.png)

📸 Proof: Full screenshots and logs are available in the repo

✅ Python GUI 6KB–10KB Proven Stable and Optimal
This Python GUI script weighs just 10KB, yet has proven capable of running large local models like 7B and 13B without errors, crashes, or freezing — even on non-GPU machines.

---

💡 Conclusion for GUI Users:

If you have RAM:

8GB: Deepseek 1.3B or TinyLlama are sufficient, and Qwen 4B, but don't expect super responsiveness.

12GB–16GB: Jump straight to Mistral 7B or DeepSeek-Coder 6.7B and the 13B Q4_K_M models.

>16GB: You can go all-in on 13B models like Yi-13B, OpenHermes 13B, etc.

---

📌 Highlights:
✔️ Super lightweight (10KB)
✔️ No complex installation
✔️ Uses llama.cpp as backend (GGUF compatible)
✔️ Stable RAM usage and RAM/CPU optimization built-in
✔️ Successfully loads 13B models
✔️ GUI remains responsive
✔️ Output is stable even with large models

💡 This GUI is even more efficient than many bulky WebUIs because it doesn’t waste system resources. Ideal for low-end PC users who want to harness the power of large local models.

---

📌 Visual Explanation (Flowchart Suggestion)
[Start GUI]
   ↓
[Load Model]
   → RAM = 15.5GB
   → CPU = 60%
   ↓
[Model Idle]
   → RAM = 12.3GB
   → CPU = 5%
   ↓
[Prompt Received]
   → CPU = 100%
   ↓
[Generate Response]
   → Output OK
   ↓
[Idle Again]
   → RAM unchanged
   → CPU drops

---

## 🇬🇧 Prompt Handling Flow (English Version)

## 🧠 Prompt Handling Flow in Multichat AI GUI

This flowchart illustrates how prompt handling works in the offline multitab AI chat GUI (Python-based):

[User] / [Pengguna]
      ↓
Click 'Start Model' / Klik 'Mulai Model'
      ↓
Model Loaded in Tab Utama / Model Dimuat di Tab Utama
      ↓
User opens New Chat Tab / Pengguna membuka Tab Chat Baru
      ↓
Send Prompt (Queued) / Kirim Prompt (Antri)
      ↓
Prompt processed sequentially / Prompt diproses secara berurutan
      ↓
AI generates response / AI menghasilkan respons
      ↓
Display response in correct tab / Tampilkan respons di tab yang sesuai

🔄 Quick Notes:
Prompts are handled one-by-one in the order they were sent.

All chat tabs are only enabled after the model is loaded from the main tab.

Each tab has its own buffer and runs via queue threads — preventing output mix-up.

---


✅ Documentation Status Recap (English Version)

| Proof Element	| Status |
|----------------|---------|
| 📜 Session logs |	✅ Available |
| 🖼️ Screenshot (model load)  |	✅ Available |
| 🖼️ Screenshot (idle)	| ✅ Available |
| 🧪 Large model (13B Q5_K_M)	 | ✅ Tested |
| ⚙️ Lightweight GUI (6KB–16KB Python)	| ✅ Works smoothly and safely with RAM/CPU optimization built-in |

---

✅ FAQ — Frequently Asked Questions (Trust Booster Edition)
❓ Can this GUI really run a 13B model without a GPU?
✅ Yes! Successfully tested with llama-2-13b-chat.Q4_K_M.gguf on:

💻 CPU: Intel i5-9400F (no iGPU)

🧠 RAM: 16GB DDR4

⚙️ Backend: llama.cpp

📦 GUI: Only a 10KB Python script

❓ Where’s the real proof?
📸 Screenshots during model load and idle are uploaded to docs/screenshots/
📄 Complete 13B model session logs available in docs/session-logs/
✅ No errors, no crashes. Just slight delay under heavy processing — perfectly normal.

❓ Is this GUI heavy?
❌ Not at all. It’s just 10KB. No bloated dependencies like Gradio or Electron.
✔️ No random ports. No tracking.
✔️ 100% offline and local.
✔️ Based purely on Tkinter.

❓ Can I use other 7B, 8B, or 13B models?
✅ Absolutely! Already tested with:

Mistral 7B

DeepSeek Coder 6.7B

DeepSeek Coder 7B

Nous Hermes 13B (Q4_K_M)

LLaMA 13B (Q4_K_M)

❓ I only have 8GB RAM, will it work?
✅ Yes, just use smaller models like:

TinyLlama 1.1B

DeepSeek Coder 1.3B

Qwen 4B

🛠️ Set max_tokens low to match your available RAM in the GUI settings.

❓ “I still don’t believe this GUI can run 13B on just 16GB RAM. Really?”
💬 “Try it yourself, bro. The repo is public 😎”

❓ “Is the GUI really that lightweight?”
✅ Yep. File size is only 10KB.
No web servers, no complex backends, no heavy libraries.

❓ “Will it crash when loading large models?”
🚫 As long as your system is stable and swap file is active, crashes are extremely rare.
📊 Even with RAM above 15GB during model load, logs show stable performance.

❓ “Is there actual proof?”
📸 Yes. Screenshots and logs are available in the docs/session-logs/ and docs/screenshots/ folders.

❓ “What if I still don’t believe?”
😎 Feel free to test it yourself. Everything is open-source.
Clone the repo now and experience it.

📜 Honest Note (for README or FAQ)
💡 So far, this GUI has only been tested with models up to 15B (Q5_K_M) due to 16GB RAM limitations and no GPU.
But the Python GUI (6KB–16KB) still handled it smoothly and efficiently — something many wouldn’t expect without a high-end setup.

For 33B+ models, testing will follow when more RAM is available. The GUI already supports CPU/RAM optimization.

---

## 📦 GUI Version List

Below are the GUI versions that have been successfully tested:


| File Version                  | Status                 | Main Features                                                          |
|----------------------------------|----------------------------------|--------------------------------------|
| llamacpp_gui_combined.py      | ✅ Stable              | Initial combined version of chat + system GUI                          |
| llamacpp_gui_combinedv2.py    | ✅                     | Added model & prompt configuration options                             |
| llamacpp_gui_combinedv3.py    | ✅                     | Minor bug fixes, cleaner UI                                            |
| llamacpp_gui_combinedv4.py    | ✅                     | + Logging support and auto-load model feature                          |
| llamacpp_gui_combinedv5.py    | ✅                     | + Chat history and system prompt support                               |
| llamacpp_gui_combinedv6.py    | ✅                     | + Theme mode and advanced configuration                                |
| llamacpp_gui_combinedv7.py    | ✅                     | + Auto-save session and repeat_penalty setting                         |
| llamacpp_gui_combinedv8.py    | ✅                     | + Auto-save model loading now works properly                           |
| llamacpp_gui_combinedv9.py    | ✅               | + “Rakyat Mode” in settings + RAM-friendly prompt + LLaMA Server       |
| llamacpp_gui_combinedv10.py    | ✅               | + Generate Prompt Words per chunk, chunk delay, auto chunk resume, auto chunkresum via prompt, chat context, save context, and load save context      |
| llamacpp_gui_combinedv11.py    | ✅ Latest              | + Memory Mode and Final Version      |
| llamacpp_multichat_ai_gui.py    | ✅ Latest              | + Memory Mode + Multi Chat Tab AI + Generate Chunk + Chat Context + Auto Save Chat Context + Auto Save Chat History + Import Prompt From File + Close Tab + Prompt System + CTX Size and  Tokens Settings  + Change Model   |
| llamacpp_rakyatmode.py        | ✅ Latest              | + --ctx-size defaults to 1024 + optimized for low RAM                  |
| llamacpp_gui_mode.py          | ✅ Experimental/Stable | Lightweight GUI mode + supports loading 13B Q4_K_M model               |
| llamacpp_gui_modev2.py        | ✅                     | Combination of GUI mode and system layout                              |

---

💡 Minimum PC Requirements

| Componentn         | Minimum Spec                    |
|------------------|-----------------------------|
| Prosesor         | i3/i5 Gen 8+ (or Ryzen 3+) |
| RAM              | 8-16 GB (32GB–64GB recommended)|
| GPU              | GTX 1650 / equivalent (VRAM 4GB) |
| OS               | Windows 10/11 64-bit        |
| Python           | 3.10+                       |

---

🤝 Contributions & Credits
👨‍💻 Creator: [satrianovian20] – Modified offline GUI using Tkinter
✍️ AI Script & Error Fix Contributions: ChatGPT
🔁 Models by Meta (LLaMA), WizardLM, and the open-source community
💡 Inspired by real-world issues faced by low-end PC users

---

📦 How to Install & Run
1. Download Python
Get Python 3.10 from: https://www.python.org/downloads/release/python-3100/

- ✅ Check "Add Python to PATH" during installation!
- pip install requests
- download from the link: (https://github.com/ggml-org/llama.cpp/releases) [llama-b59xx-bin-win-cpu-x64 atau llama-b5899-bin-win-cpu-x64]
- place the python gui in the llamacpp folder downloaded from the link: (https://github.com/ggml-org/llama.cpp/releases)
- Run the python gui by double-clicking it
> Note: Just look for a working Llama CPP build to load AI models in link: (https://github.com/ggml-org/llama.cpp/releases). Typically, the latest ones are llama-b59xx-bin-win-cpu-x64 or llama-b5899-bin-win-cpu-x64. The point is that the model loading feature in the GUI is fully functional but llamacpp as a backend is not functioning properly so please look for llama-b5899-bin-win-cpu-x64 - llama-b59xx-bin-win-cpu-x64 which can load the model.

---

💖 Support This Project
If this project helped you and you'd like to support its development, you can donate via:

- 💸 [Saweria](https://saweria.co/satrianovian20)
- ☕ [PayPal](https://www.paypal.com/paypalme/satrianovian)

Thank you so much for your support! 🙏

---

☕ Final Note
This project was born from limitation and turned into innovation.
Like you, I struggled with WebUIs from GitHub that just wouldn’t run.
So I:

Built my own GUI

With just a single .py file

For AI Chat & Coding

While still able to watch YouTube or music videos in 720p 😄

💬 If this helped you, consider starring the repo!
Let’s make sure no one ever has to say “My AI won’t start...” again 😅
