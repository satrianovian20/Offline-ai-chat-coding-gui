# ğŸ§  Offline AI Chat & Coding GUI (Rakyat Edition)
GUI Python ringan (Tkinter) untuk menjalankan AI chat &amp; coding dengan model GGUF lokal (Low GUI PYTHON from Tkinter to run Offline AI Chat and Coding)

Moto:
GUI Rakyat Edition: Ringan, Garang, Tak Terbendung, AI GUI Bebas Crashâ„¢, Anti-Bluescreen Framework, The GUI That Refused to Die

ğŸš€ Cloned already? Donâ€™t forget to â­ if this project made your CPU smile ğŸ˜„  "ğŸ§ª Verified Stress Test Logs (Real-World AI Usage by Rakyat CPU)" ğŸ† "LLaMA.CPP Rakyat Edition: ChatGPT Experience in kilobytes." ğŸ† "Your Own GPT Pro: Local, Open-Source & Fully Customizable" ğŸ’» "Stress-Tested Terminator AI GUI" ğŸš§ "Born in CPU Hell, Forged by Low-End Desperation" ğŸ® "I just wanted to pirate a game..." âš™ï¸ "...but accidentally created a multiversally-compatible AI GUI." "This repo is so stable, even bugs are afraid to appear." ğŸğŸ’€ *Powered by Bugs, Sustained by Chaos*

---

-------------------------------------------

ğŸš¨ PROYEK INI SUDAH TIDAK GRATIS ğŸš¨

GUI AI Chatbot & Multichat sekarang hanya tersedia versi **berbayar murah meriah**  
â¡ï¸ **Sekali beli, tanpa perpanjangan license.** 

Dapatkan di sini ğŸ‘‰ [Kilashare](https://kilashare.blogspot.com/)

ğŸ’– **Kenapa Berbayar?**  
Proyek ini butuh biaya untuk tetap berjalan. Kalau kamu suka dengan GUI ini dan ingin support pengembangannya, silakan beli lisensinya.

"Source code ini bukan versi final, compile ke EXE butuh setup khusus & modular stress test. Versi EXE hanya tersedia lewat pembelian resmi."

â€œBikin EXE sendiri butuh setup environment, debug modular, dan stress test berjam-jam. Dengan harga cuma $5 (setara 1x nongkrong), kamu langsung dapat versi stabil siap pakai.â€

â€œVersi EXE sudah dites semua fitur aktif â†’ Chunk Resume, Auto Save, Role Mode, Stop Prompt. Fitur ini tidak aktif penuh kalau hanya pakai source code.â€

ğŸ‘‰ â€œKalian cloning repo sekali, compile sekali, error sekali. Saya compile + stress test 3 layer (py, compile, exe), hasilnya produk anti-crash. Bayar murah aja dapet yang udah siap pakai, hemat waktu debugging berminggu-minggu.â€

ğŸ“œ Chaos Dev Stress Test Protocolâ„¢
Setiap EXE lulus uji 3 lapis stress test sebelum dilepas:

ğŸ§ª Python Script Stress Test â†’ spam input, test memory leak, dan handling teks panjang.

ğŸ§ª Compile Stress Test â†’ build .exe berulang kali pakai setup modular â†’ cek crash/bug tersembunyi.

ğŸ§ª Exe Runtime Stress Test â†’ dijalankan di PC low-end sampai high-end â†’ spam fitur (resume, role, autosave, stop).

âš”ï¸ Hasil:

Anti-Crashâ„¢ Certified

Kompatibel universal dengan semua build llama-server.exe

Stable Mode (fitur aktif penuh hanya di versi EXE resmi)

ğŸ™ Terima kasih sudah mendukung chaos developer!  

## ğŸ’– Dukung Proyek Ini

Jika kamu merasa proyek ini bermanfaat dan ingin mendukung pengembangannya, kamu bisa berdonasi melalui:
ğŸ‘‰ Untuk mendapatkan source code & file GUI:

- ğŸ’¸ [Saweria](https://saweria.co/satrianovian20)
- â˜• [PayPal](https://www.paypal.com/paypalme/satrianovian)


**Bayar dulu, baru dapat akses download**
Terima kasih banyak atas dukungannya! ğŸ™

-------------------------------------------

ğŸš¨ THIS PROJECT IS NO LONGER FREE ğŸš¨

The AI Chatbot & Multichat GUI is now available only in a **cheap paid version**
â¡ï¸ **One-time purchase, no license renewals.**

Get it here ğŸ‘‰ [Kilashare](https://kilashare.blogspot.com/)

ğŸ’– **Why Paid?**  
This project needs support to keep development going. If you find it useful, please consider supporting by purchasing a license.

"This source code is not the final version, compiling to EXE requires special setup & modular stress testing. The EXE version is only available through official purchase."

"Creating your own EXE requires hours of environment setup, modular debugging, and stress testing. For just $5 (the equivalent of one visit), you get a ready-to-use, stable version."

"The EXE version has been tested with all active features â†’ Chunk Resume, Auto Save, Role Mode, Stop Prompt. These features are not fully active if you only use the source code."

ğŸ‘‰ â€œYou clone the repo once, compile once, and crash once. I compiled and stress-tested 3 layers (py, compile, exe), resulting in a crash-proof product. Pay less and get a ready-to-use product, saving you weeks of debugging time.â€

ğŸ“œ Chaos Dev Stress Test Protocolâ„¢
Every EXE passes a three-layer stress test before release:

ğŸ§ª Python Script Stress Test â†’ input spam, memory leak test, and long text handling.

ğŸ§ª Compile Stress Test â†’ build the .exe repeatedly using a modular setup â†’ check for hidden crashes/bugs.

ğŸ§ª Exe Runtime Stress Test â†’ run on low-end to high-end PCs â†’ feature spam (resume, role, autosave, stop).

âš”ï¸ Results:

Anti-Crashâ„¢ Certified

Universally compatible with all llama-server.exe builds

Stable Mode (full features enabled only in the official EXE version)

ğŸ™ Thank you for supporting chaos developers!  

ğŸ’– Support This Project
If this project helped you and you'd like to support its development, you can donate via:
ğŸ‘‰ To get the source code, you can donate via:

- ğŸ’¸ [Saweria](https://saweria.co/satrianovian20)
- â˜• [PayPal](https://www.paypal.com/paypalme/satrianovian)

**Pay first, then receive download access**
Thank you so much for your support! ğŸ™

-------------------------------------------

> ğŸ‡®ğŸ‡© This project is primarily documented in Indonesian.
> ğŸ‡¬ğŸ‡§ English overview is provided below.
> â€œThis project is based on the original LLaMA GUI by Satria Novian â€“ [GitHub link], licensed under the MIT License. Copyright Â© 2025 Satria Novian.â€

-------------------------------------------
Portofolio Chatbot GUI/ Chatbot WebUI
# ğŸ“¸ Screenshot:
![Screenshot](https://github.com/satrianovian20/Offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-09-19%20212225.png)

![Screenshot](https://github.com/satrianovian20/Offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-09-18%20194415.png)

![Screenshot](https://github.com/satrianovian20/Offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-09-16%20192713.png)

![Screenshot](https://github.com/satrianovian20/Offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-09-16%20192720.png)

![Screenshot](https://github.com/satrianovian20/Offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-09-19%20201829.png)

![Screenshot](https://github.com/satrianovian20/Offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-09-19%20212225.png)

![Screenshot](https://github.com/satrianovian20/Offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-09-16%20193110.png)

![Screenshot](https://github.com/satrianovian20/Offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-09-16%20193118.png)

![Screenshot](https://raw.githubusercontent.com/USERNAME/REPO/main/assets/screenshot.png)

![Screenshot](https://raw.githubusercontent.com/USERNAME/REPO/main/assets/screenshot.png)

# ğŸ¥ Video :


ğŸ‡®ğŸ‡© Bahasa Indonesia:
âœ… FAQ â€” Pertanyaan Umum (Trust Booster Edition) 

â“ GUI ini beneran bisa jalanin model 13B tanpa GPU? Ya! Sudah diuji langsung dengan model llama-2-13b-chat.Q4_K_M.gguf di sistem dengan:

ğŸ’» CPU: Intel i5-9400F (tanpa iGPU)

ğŸ§  RAM: 16GB DDR4

âš™ï¸ Backend: llama.cpp

ğŸ“¦ GUI: Llamacpp AI Chatbot GUI

â“ Bukti nyatanya mana? ğŸ“¸ Screenshot saat load model dan idle sudah diunggah di folder docs/screenshots/

ğŸ“„ Log lengkap sesi percobaan model 13B tersedia di docs/session-logs/

Tidak ada error, tidak crash, hanya delay wajar saat proses berat.

â“ GUI-nya berat gak? Tidak. GUI ini hanya 10KB, tanpa dependensi besar seperti Gradio atau Electron.

Tidak buka port aneh-aneh.

Tidak ada tracking.

Murni offline dan lokal.

UI sangat ringan, hanya berbasis tkinter.

â“ Bisa pakai model 7B, 8B, atau 13B lain? Bisa! Sudah diuji dengan:

Mistral 7B

DeepSeek Coder 6.7B

DeepSeek Coder 7B

Nous Hermes 13B (Q4_K_M)

LLaMA 13B (Q4_K_M)

â“ RAM saya cuma 8GB, bisa jalan? Bisa, asal model yang dipilih sesuai. Gunakan model kecil seperti:

TinyLlama 1.1B Q8_0

DeepSeek Coder 1.3b Q8_0

Mistral 4B Q8_0

Open Hermes 7B Q4_K_M

Atur max_tokens di GUI agar tidak melebihi kapasitas RAM kamu.

â“â€œSaya masih nggak percaya GUI ini bisa jalanin model 13B cuma dengan RAM 16GB. Beneran bisa?â€ ğŸ’¬ â€œCoba sendiri aja bro ğŸ˜â€

â“â€œEmang GUI-nya ringan banget ya?â€ âœ… Iya. Ukuran file .py cuma mb. Gak ada embel-embel web server, backend rumit, atau library berat.

â“â€œBisa crash gak pas load model besar?â€ ğŸš« Selama sistem kamu stabil dan swap file aktif, hampir nggak pernah crash. Bahkan log menunjukkan performa tetap normal walau RAM di atas 15GB pas awal load.

â“â€œAda buktinya?â€ ğŸ“¸ Sudah ada screenshot dan log di folder docs/session-logs/ dan docs/screenshots/.

â“â€œKalau saya nggak percaya tetap?â€ ğŸ˜ Silakan buktikan sendiri.


ğŸ‡¬ğŸ‡§ English Version:

âœ… FAQ â€” Frequently Asked Questions (Trust Booster Edition) 

â“ Can this GUI really run a 13B model without a GPU? âœ… Yes! Successfully tested with llama-2-13b-chat.Q4_K_M.gguf on:

ğŸ’» CPU: Intel i5-9400F (no iGPU)

ğŸ§  RAM: 16GB DDR4

âš™ï¸ Backend: llama.cpp

ğŸ“¦ GUI: Llamacpp AI Chatbot GUI

â“ Whereâ€™s the real proof? ğŸ“¸ Screenshots during model load and idle are uploaded to docs/screenshots/ ğŸ“„ Complete 13B model session logs available in docs/session-logs/ âœ… No errors, no crashes. Just slight delay under heavy processing â€” perfectly normal.

â“ Is this GUI heavy? âŒ Not at all. Itâ€™s just 10KB. No bloated dependencies like Gradio or Electron. âœ”ï¸ No random ports. No tracking. âœ”ï¸ 100% offline and local. âœ”ï¸ Based purely on Tkinter.

â“ Can I use other 7B, 8B, or 13B models? âœ… Absolutely! Already tested with:

Mistral 7B

DeepSeek Coder 6.7B

DeepSeek Coder 7B

Nous Hermes 13B (Q4_K_M)

LLaMA 13B (Q4_K_M)

â“ I only have 8GB RAM, will it work? âœ… Yes, just use smaller models like:

TinyLlama 1.1B Q8_0

DeepSeek Coder 1.3b Q8_0

Mistral 4B Q8_0

Open Hermes 7B Q4_K_M

ğŸ› ï¸ Set max_tokens low to match your available RAM in the GUI settings.

â“ â€œI still donâ€™t believe this GUI can run 13B on just 16GB RAM. Really?â€ ğŸ’¬ â€œTry it yourself, bro. ğŸ˜â€

â“ â€œIs the GUI really that lightweight?â€ âœ… Yep. File size is only mb. No web servers, no complex backends, no heavy libraries.

â“ â€œWill it crash when loading large models?â€ ğŸš« As long as your system is stable and swap file is active, crashes are extremely rare. ğŸ“Š Even with RAM above 15GB during model load, logs show stable performance.

â“ â€œIs there actual proof?â€ ğŸ“¸ Yes. Screenshots and logs are available in the docs/session-logs/ and docs/screenshots/ folders.

â“ â€œWhat if I still donâ€™t believe?â€ ğŸ˜ Feel free to test it yourself. 

