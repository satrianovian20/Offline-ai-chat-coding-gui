# ğŸ§  Offline AI Chat & Coding GUI (Rakyat Edition)
GUI Python ringan (Tkinter) untuk menjalankan AI chat &amp; coding dengan model GGUF lokal (Low GUI PYTHON from Tkinter to run Offline AI Chat and Coding)

ğŸ‡®ğŸ‡© Bahasa Indonesia:

ğŸ‰ Pertama kalinya dalam sejarah: Model AI 13B (Q4_K_M) berhasil dijalankan hanya dengan RAM 16GB â€” tanpa GPU, tanpa web server, cukup dengan GUI Python ukuran 10KB! Bukti nyata efisiensi maksimal ğŸ’ª
âš ï¸ Catatan RAM 16GB:

Meskipun sistem Anda 16GB, Windows dan Office 2024 bisa menyita 3â€“5 GB di background. Namun, GUI ini tetap dapat menjalankan model 7B - 13B Q4_K_M. Tanpa GPU, tanpa swap besar.

## âš–ï¸ Perbandingan GUI Python 6KB-16kb vs WebUI Berat

| **Fitur / Aspek**                | ğŸ **GUI Python 6KB-16KB (tkinter)** | ğŸŒ **WebUI (Gradio/Oobabooga dll.)** |
|----------------------------------|----------------------------------|--------------------------------------|
| âœ… Ukuran File GUI               | **10 KB**                        | > **100 MB**                         |
| âš™ï¸ Bahasa Pemrograman            | Python (tkinter native)          | Python + Gradio + JS + HTML          |
| ğŸ§  Model yang Diuji              | 7B, 13B (Q4_K_M)                 | 7B, 13B                               |
| ğŸ§® RAM Minimum (7B - 13B Q4_K_M)      | **12.3 â€“ 15.5 GB**               | **> 18 â€“ 20 GB**                      |
| ğŸ–¥ï¸ CPU Pengujian                | i5-9400F (no GPU)                | Biasanya pakai GPU / CPU kuat        |
| ğŸš€ Waktu Load Model 13B         | **35 â€“ 40 detik**                | Bisa > 1 menit                       |
| ğŸŸ¢ RAM Saat Idle                | 12.1 â€“ 12.4 GB                   | > 15 GB                               |
| ğŸ› ï¸ Konfigurasi Awal            | Hanya 1 file `.py`               | Banyak dependensi dan setup venv     |
| ğŸ“‰ Risiko Error/Crash           | **Sangat rendah / stabil**       | Kadang freeze / error token          |
| ğŸª„ Token yang Diuji Lancar      | 5000 token                | Bergantung setting/model             |
| ğŸ“¡ Server Web                   | **Tidak perlu**                  | Wajib buka server (http/websocket)   |
| ğŸ§© Dukungan Plugin              | Manual (custom)                  | Banyak tapi berat                    |
| ğŸ’¬ Chat & Coding Mode           | âœ… Sangat cocok                  | âœ… Cocok, tapi lebih berat            |
| ğŸ¤¯ Respons Not Responding?     | Hanya saat proses berat dan tanpa crash         | Sering delay jika RAM kritis         |

ğŸ‡¬ğŸ‡§ English Version:

ğŸ‰ First time in history: A 13B (Q4_K_M) AI model runs smoothly with just 16GB RAM â€” no GPU, no web server, only a 6KB - 16kb Python GUI! Proof of ultimate efficiency ğŸ’ª

âš ï¸ 16GB RAM Note:

Even if your system has 16GB, Windows and Office 2024 can take up 3â€“5GB in the background. However, this GUI can still run the 7B - 13B Q4_K_M model efficiently. No GPU, no large swap requirements.

> ğŸ‡®ğŸ‡© This project is primarily documented in Indonesian.
> ğŸ‡¬ğŸ‡§ English overview is provided below.

## âš–ï¸ Comparison: Python GUI (6â€“16KB) vs Heavy WebUIs

| **Feature / Aspect**             | ğŸ **Python GUI 6KB-16KB (Tkinter)**       | ğŸŒ **WebUIs (Gradio/Oobabooga, etc.)**     |
|:----------------------------------|:----------------------------------------|:--------------------------------------------|
| âœ… **GUI File Size**              | **10 KB**                               | > **100 MB**                                 |
| âš™ï¸ **Programming Language**       | Native Python (Tkinter)                 | Python + Gradio + JS + HTML                  |
| ğŸ§  **Tested Model Types**         | 7B, 13B (Q4_K_M)                        | 7B, 13B                                       |
| ğŸ§® **Minimum RAM (7Bâ€“13B Q4_K_M)**| **12.3 â€“ 15.5 GB**                      | **> 18 â€“ 20 GB**                              |
| ğŸ–¥ï¸ **Test CPU**                   | i5-9400F (no GPU)                       | Typically uses GPU or high-end CPU           |
| ğŸš€ **Model Load Time (13B)**      | **35 â€“ 40 seconds**                     | Can take > 1 minute                          |
| ğŸŸ¢ **Idle RAM Usage**             | 12.1 â€“ 12.4 GB                          | > 15 GB                                       |
| ğŸ› ï¸ **Initial Setup**             | Single `.py` file only                  | Many dependencies and venv setup             |
| ğŸ“‰ **Error / Crash Risk**         | **Very low / stable**                   | Sometimes freezes or token errors            |
| ğŸª„ **Smooth Token Output Tested** | 5000 tokens                             | Varies by settings/model                     |
| ğŸ“¡ **Web Server Required**        | **Not needed**                          | Required (HTTP/websocket server)             |
| ğŸ§© **Plugin Support**             | Manual (customizable)                   | Many, but resource-heavy                     |
| ğŸ’¬ **Chat & Coding Mode**         | âœ… Highly suitable                      | âœ… Suitable, but heavier                      |
| ğŸ¤¯ **"Not Responding" Behavior**  | Only during heavy loads, no crash       | Frequent delays when RAM is low              |


## ğŸ“¸ Screenshot GUI From llamacpp_gui_combinedv9.py

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-18%20160408.png)

## ğŸ“¸ Screenshot GUI From llamacpp_gui_mode.py (Experiment GUI)

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-19%20110119.png)

## ğŸ“¸ Screenshot Chat From GUI
![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-18%20170753.png)

## ğŸ“¸ Second Screenshot Chat From GUI
![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-18%20170809.png)

---

## ğŸ‡®ğŸ‡© Bahasa Indonesia

GUI lokal untuk LLaMA.cpp:

ğŸ“ Cocok untuk chat dan coding!

ğŸ’¬ Contoh jawaban untuk user awam:
â€œGUI ini nggak ngebatesin kemampuan AI-nya. Kamu bisa atur output token sesuai kemampuan RAM PC kamu. Misal RAM kamu cuma 8GB, kamu bisa atur max token ke 150 biar tetap lancar. Kalau RAM kamu 16GB, bisa gaspol sampai 10.000 token. Jadi fleksibel, ringan, dan tetap powerful!â€

## ğŸ“¸ Screenshot (Model berhasil dimuat di RAM 16GB)

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-17%20114247.png)

## ğŸ“¸ Screenshot (Model berhasil dimuat di RAM 16GB)

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-17%20125025.png)

ğŸ“¸ Bukti: Sudah dilampirkan screenshot dan log lengkap di repo

ğŸ’¡ Kesimpulan Buat Pengguna GUI:
Kalau user punya RAM:

8GB: Deepseek 1.3B atau TinyLlama cukup, tapi jangan berharap jawaban super.

12GBâ€“16GB: Langsung lompat ke Mistral 7B atau DeepSeek-Coder 6.7B.

>16GB: Bisa gas model 13B seperti Yi-13B, OpenHermes 13B, dll.

Optimalitas GUI Python 6KB-10kb:
âœ… GUI Python 10KB Terbukti Optimal dan Stabil

GUI ini hanya berukuran 10KB namun sudah terbukti mampu menangani model AI lokal besar seperti 7B dan 13B tanpa eror, crash, atau freeze, bahkan saat dijalankan di PC tanpa GPU.

ğŸ“Œ Keunggulan:

âœ”ï¸ Ukuran super ringan (10KB)

âœ”ï¸ Bisa langsung jalan tanpa instalasi ribet

âœ”ï¸ Menggunakan llama.cpp sebagai backend (kompatibel GGUF)

âœ”ï¸ RAM usage stabil di ~12.3GB saat idle

âœ”ï¸ Load model 13B sukses tanpa error

âœ”ï¸ GUI tetap responsif dan tidak berat

âœ”ï¸ Output respons tetap stabil meski model besar

ğŸ’¡ GUI ini bahkan lebih optimal dibanding banyak UI besar di luar sana karena tidak menyia-nyiakan resource, cocok untuk pengguna low-end PC yang tetap ingin merasakan kekuatan model besar lokal.

ğŸ“Œ Bonus Penjelasan Visual (Flowchart Saran)

csharp
Copy
Edit
[Mulai GUI]
   â†“
[Load Model]
   â†’ RAM = 15.5GB
   â†’ CPU 60%
   â†“
[Model Idle]
   â†’ RAM = 12.3GB
   â†’ CPU 5%
   â†“
[Prompt Masuk]
   â†’ CPU 100%
   â†“
[Generate Response]
   â†’ Output OK
   â†“
[Idle Lagi]
   â†’ RAM tetap
   â†’ CPU turun

âœ… Recap Status Dokumentasi (versi Indonesia):
Elemen Bukti	Status
ğŸ“œ Log sesi (session-logs)	âœ… Sudah ada
ğŸ–¼ï¸ Screenshot saat load model	âœ… Sudah ada
ğŸ–¼ï¸ Screenshot saat idle	âœ… Sudah ada
ğŸ§ª Model besar (13B Q4_K_M)	âœ… Sudah diuji
âš™ï¸ GUI ringan (6KB-16kb Python script)	âœ… Terpakai dengan lancar dan tanpa crash karena adanya fitur otomatis yang melakukan optimaliasi ram dan cpu

âœ… FAQ â€” Pertanyaan Umum (Trust Booster Edition)
â“ GUI ini beneran bisa jalanin model 13B tanpa GPU?
Ya! Sudah diuji langsung dengan model llama-2-13b-chat.Q4_K_M.gguf di sistem dengan:

ğŸ’» CPU: Intel i5-9400F (tanpa iGPU)

ğŸ§  RAM: 16GB DDR4

âš™ï¸ Backend: llama.cpp

ğŸ“¦ GUI: Python script ukuran hanya 10KB

â“ Bukti nyatanya mana?
ğŸ“¸ Screenshot saat load model dan idle sudah diunggah di folder docs/screenshots/

ğŸ“„ Log lengkap sesi percobaan model 13B tersedia di docs/session-logs/

Tidak ada error, tidak crash, hanya delay wajar saat proses berat.

â“ GUI-nya berat gak?
Tidak. GUI ini hanya 10KB, tanpa dependensi besar seperti Gradio atau Electron.

Tidak buka port aneh-aneh.

Tidak ada tracking.

Murni offline dan lokal.

UI sangat ringan, hanya berbasis tkinter.

â“ Bisa pakai model 7B, 8B, atau 13B lain?
Bisa! Sudah diuji dengan:

Mistral 7B

DeepSeek Coder 6.7B

DeepSeek Coder 7B

Nous Hermes 13B (Q4_K_M)

LLaMA 13B (Q4_K_M)

â“ RAM saya cuma 8GB, bisa jalan?
Bisa, asal model yang dipilih sesuai. Gunakan model kecil seperti:

TinyLlama 1.1B

DeepSeek Coder 1.3B

Atur max_tokens di GUI agar tidak melebihi kapasitas RAM kamu.

â“â€œSaya masih nggak percaya GUI ini bisa jalanin model 13B cuma dengan RAM 16GB. Beneran bisa?â€
ğŸ’¬ â€œCoba sendiri aja bro, repo udah public kok ğŸ˜â€

â“â€œEmang GUI-nya ringan banget ya?â€
âœ… Iya. Ukuran file .py cuma 10KB. Gak ada embel-embel web server, backend rumit, atau library berat.

â“â€œBisa crash gak pas load model besar?â€
ğŸš« Selama sistem kamu stabil dan swap file aktif, hampir nggak pernah crash. Bahkan log menunjukkan performa tetap normal walau RAM di atas 15GB pas awal load.

â“â€œAda buktinya?â€
ğŸ“¸ Sudah ada screenshot dan log di folder docs/session-logs/ dan docs/screenshots/.

â“â€œKalau saya nggak percaya tetap?â€
ğŸ˜ Silakan buktikan sendiri. Semuanya open source. Mau test sendiri? Silakan kloning repo-nya sekarang.

Catatan Jujur (untuk README atau FAQ)
ğŸ’¡ Saat ini, pengujian terbatas pada model hingga 13B (Q4_K_M) karena sistem hanya memiliki RAM 16GB tanpa GPU.
Namun, GUI Python 6KB-16KB ini berhasil menangani model besar tersebut secara stabil dan efisien, yang biasanya tidak mungkin dilakukan tanpa sistem high-end.
Untuk model 33B ke atas, uji coba akan dilakukan jika tersedia perangkat dengan RAM lebih besar. Ditambah bisa melakukan optimaliasi ram dan cpu.

---

ğŸ¤ Kontribusi & Kredit
ğŸ‘¨â€ğŸ’» Creator: [satrianovian20] â€“ Modifikasi GUI offline dengan Tkinter

âœï¸ Kontributor AI Script & Fixer Error: ChatGPT

ğŸ” Model by Meta (LLaMA), WizardLM, dan komunitas open-source

ğŸ’¡ Terinspirasi oleh kesulitan real pengguna dengan PC low-end

---
## ğŸ“¦ Daftar Versi GUI

Berikut versi-versi GUI yang berhasil diuji:

| Versi File                 | Status  | Fitur Utama                             |
|---------------------------|---------|------------------------------------------|
| llamacpp_gui_combined.py  | âœ… Stabil | Versi awal gabungan GUI chat + sistem    |
| llamacpp_gui_combinedv2.py| âœ…        | Tambahan pengaturan model & prompt       |
| llamacpp_gui_combinedv3.py| âœ…        | Fix error kecil, UI lebih bersih         |
| llamacpp_gui_combinedv4.py| âœ…        | + Logging dan auto-load model            |
| llamacpp_gui_combinedv5.py| âœ…        | + Riwayat chat dan sistem prompt         |
| llamacpp_gui_combinedv6.py| âœ…        | + Theme mode dan pengaturan lanjutan     |
| llamacpp_gui_combinedv7.py| âœ…        | + Auto-save session dan repeat_penalty   |
| llamacpp_gui_combinedv8.py| âœ…       | + Auto-save load model berfungsi        |
| llamacpp_gui_combinedv9.py| âœ… Terbaru       | + Fitur Rakyat Mode di menu pengaturan dan prompt hemat ram di samping mulai Llama Server        |
| llamacpp_rakyatmode.py| âœ… Terbaru | + --ctx-size default ke 1024 + Lowram        |
| llamacpp_gui_mode.py      | âœ… Eksperimen + Stabil | Mode GUI ringan eksperimen + load model 13 Q4_K_M               |
| llamacpp_gui_modev2.py    | âœ…        | Kombinasi GUI mode dengan layout sistem  |

## ğŸ’¡ Syarat Minimum PC

| Komponen         | Minimum                    |
|------------------|-----------------------------|
| Prosesor         | i3/i5 Gen 8+ (atau Ryzen 3+) |
| RAM              | 8-16 GB (direkomendasikan 32 GB - 64 gb)|
| GPU              | GTX 1650 / setara (VRAM 4GB) |
| OS               | Windows 10/11 64-bit        |
| Python           | 3.10+                       |

---

## ğŸ“¦ Cara Install & Jalankan

### 1. Download Python
Install Python 3.10 dari https://www.python.org/downloads/release/python-3100/

> âœ… Centang â€œAdd Python to PATHâ€ saat install!
> pip install requests
> Jalankan gui python dengan klik dua kali

---

### ğŸ’– Dukung Proyek Ini

Jika kamu merasa proyek ini bermanfaat dan ingin mendukung pengembangannya, kamu bisa berdonasi melalui:

- ğŸ’¸ [Saweria](https://saweria.co/satrianovian20)
- â˜• [PayPal](https://www.paypal.com/paypalme/satrianovian)

Terima kasih banyak atas dukungannya! ğŸ™

### 2. Clone atau Download Proyek Ini

```bash
git clone https://github.com/satrianovian20/offline-ai-chat-coding-gui.git
cd offline-ai-chat-coding-gui

â˜• Penutup
Proyek ini dibuat karena keterbatasan memunculkan inovasi. Saya juga pernah pusing karena WebUI dari GitHub gagal jalan, sampai akhirnya saya:

Buat GUI sendiri

Cuma pakai 1 file Python untuk AI Chat & Coding

Bisa sambil nonton YouTube dan nonton video musik 720p ğŸ˜„

ğŸ’¬ Kalau kamu juga merasakan manfaatnya, bantu bintang repo ini!
Supaya tidak ada lagi yang pusing karena â€œAI-nya gak bisa jalan...â€ ğŸ˜…

---

## ğŸŒ English (International)

ğŸ‡¬ğŸ‡§ English Version:

ğŸ–¥ï¸ Local GUI for LLaMA.cpp:

ğŸ“ Perfect for Chat and Coding!

ğŸ’¬ Example Explanation for Non-Technical Users:

"This GUI doesnâ€™t limit the AIâ€™s capabilities. You can adjust the output tokens according to your PCâ€™s RAM capacity. For example, if you only have 8GB RAM, set max tokens to 150 to stay smooth. If you have 16GB RAM, you can go all out up to 10,000 tokens. So itâ€™s flexible, lightweight, and still powerful!"

ğŸ“¸ Screenshot (Model successfully loaded in 16GB RAM)





ğŸ“¸ Proof: Screenshots and complete logs are provided in the repo

ğŸ’¡ Summary for GUI Users:

If you have RAM:

8GB: Deepseek 1.3B or TinyLlama is enough (basic answers)

12GBâ€“16GB: Jump directly to Mistral 7B or DeepSeek-Coder 6.7B



16GB: Run 13B models like Yi-13B, OpenHermes 13B, etc.

âœ… Python GUI 6KBâ€“10KB Optimization:

âœ… Proven optimal and stable even for large local AI models (7B, 13B)

âœ… No GPU needed

âœ… No crashes or freezes during usage

ğŸ“Œ Highlights:

âœ”ï¸ Ultra lightweight (10KB only)

âœ”ï¸ No complicated installation required

âœ”ï¸ Uses llama.cpp as backend (GGUF compatible)

âœ”ï¸ Stable RAM usage (~12.3GB idle)

âœ”ï¸ 13B model loads without error

âœ”ï¸ Responsive GUI

âœ”ï¸ Stable output even for large models

ğŸ’¡ This GUI is even more optimized than many popular UIs out there. It makes smart use of resources and is ideal for low-end PC users wanting to use large local models.

ğŸ“Œ Bonus Visual Explanation (Suggested Flowchart)

[Start GUI]
  -> [Load Model]
       -> RAM = 15.5GB
       -> CPU = 60%
  -> [Model Idle]
       -> RAM = 12.3GB
       -> CPU = 5%
  -> [Prompt Received]
       -> CPU = 100%
  -> [Generate Response]
       -> Output OK
  -> [Idle Again]
       -> RAM steady
       -> CPU low

âœ… Documentation Status (Indonesian Version):

Evidence Element

Status

ğŸ“œ Session logs

âœ… Available

ğŸ–¼ï¸ Screenshot model loaded

âœ… Available

ğŸ–¼ï¸ Screenshot idle

âœ… Available

ğŸ§ª 13B Model (Q4_K_M) Tested

âœ… Yes

âš™ï¸ Lightweight GUI (6KB-16KB)

âœ… Works well with auto RAM/CPU optimization

âœ… FAQ â€” General Questions (Trust Booster Edition)

Q: Can this GUI really run 13B models without GPU?

A: Yes! Tested with llama-2-13b-chat.Q4_K_M.gguf on:

ğŸ’» CPU: Intel i5-9400F (no iGPU)

ğŸ§  RAM: 16GB DDR4

âš™ï¸ Backend: llama.cpp

ğŸ“¦ GUI: 10KB Python script

Q: Is there any real proof?

A: âœ… Screenshots and logs in docs/screenshots/ and docs/session-logs/

Q: Is the GUI heavy?

A: ğŸš« No. Itâ€™s only 10KB. No heavy dependencies like Gradio or Electron.

Q: Is it offline and secure?

A: âœ… Yes. No servers, no tracking. 100% local and offline.

Q: Can I use other models (7B, 8B, 13B)?

A: âœ… Yes! Tested with:

Mistral 7B

DeepSeek Coder 6.7B / 7B

Nous Hermes 13B (Q4_K_M)

LLaMA 13B (Q4_K_M)

Q: I only have 8GB RAM. Can I still use it?

A: âœ… Yes, just use smaller models like TinyLlama 1.1B or DeepSeek 1.3B.

Q: I still donâ€™t believe 13B can run on 16GB RAM!

A: ğŸ˜ Try it yourself. The repo is public!

Q: Is the GUI really that light?

A: âœ… Yep. Just 10KB .py file. No web server, no backend, no bloat.

Q: Will it crash with big models?

A: ğŸš« Rarely. As long as your system is stable and has swap active, it's fine.

Q: Is there proof?

A: ğŸ“¸ Yes. Logs and screenshots included.

Q: Still not convinced?

A: ğŸ˜ Clone it and prove it yourself. Itâ€™s all open source.

ğŸ“Œ Honest Note (README or FAQ Section)

ğŸ’¡ So far, tests have been conducted up to 13B (Q4_K_M) because the system has only 16GB RAM and no GPU.
But this 6â€“16KB Python GUI has proven to run large models efficiently without crashes.

Tests for 33B models will be done when more RAM is available.

ğŸ¤ Contributions & Credits

ğŸ‘¨â€ğŸ’» Creator: [satrianovian20] â€” Offline GUI with Tkinter

âœï¸ AI Scripting & Bug Fixing: ChatGPT

ğŸ” Models: Meta (LLaMA), WizardLM, open-source community

ğŸ’¡ Inspired by real low-end PC users struggling with bloated UIs

ğŸ“¦ Tested GUI Versions

GUI File Version

Status

Key Features

llamacpp_gui_combined.py

âœ… Stable

Initial combined Chat + System GUI

llamacpp_gui_combinedv2.py

âœ…

Model & prompt settings added

llamacpp_gui_combinedv3.py

âœ…

Minor bug fixes, cleaner UI

llamacpp_gui_combinedv4.py

âœ…

+ Logging and auto model load

llamacpp_gui_combinedv5.py

âœ…

+ Chat history + system prompt

llamacpp_gui_combinedv6.py

âœ…

+ Theme mode & advanced settings

llamacpp_gui_combinedv7.py

âœ…

+ Auto-save sessions, repeat_penalty config

llamacpp_gui_combinedv8.py

âœ…

+ Auto-load model fix

llamacpp_gui_combinedv9.py

âœ… Latest

+ Rakyat Mode menu + memory-saving prompt

llamacpp_rakyatmode.py

âœ… Latest

+ --ctx-size default 1024 + LowRAM mode

llamacpp_gui_mode.py

âœ… Stable

Experimental GUI Mode, loads 13B Q4_K_M

llamacpp_gui_modev2.py

âœ…

GUI Mode + layout integration

ğŸ’¡ Minimum System Requirements

Component

Minimum Specs

Processor

i3/i5 Gen 8+ or Ryzen 3+

RAM

8â€“16 GB (32GBâ€“64GB recommended)

GPU

GTX 1650 or equivalent (VRAM 4GB)

OS

Windows 10/11 64-bit

Python

Version 3.10+

ğŸš€ How to Install & Run

1. Download Python

Install Python 3.10 from: https://www.python.org/downloads/release/python-3100/

âœ… Check "Add Python to PATH" during installation!

Install required dependency:

pip install requests

Run the Python GUI by double-clicking the .py file.

ğŸ’– Support This Project

If this project helps you and you'd like to support its development:

- ğŸ’¸ [Saweria](https://saweria.co/satrianovian20)
- â˜• [PayPal](https://www.paypal.com/paypalme/satrianovian)

Thank you so much for your support! ğŸ™

â˜• Final Words

This project was born out of frustration and turned into innovation.

I used to struggle with WebUIs that refused to run...

So I:

Built my own GUI

Only used 1 Python file for AI Chat & Coding

Still watch YouTube and 720p music videos while it's running ğŸ˜„

ğŸ’¬ If this project helps you too, please star the repo!
So nobody ever says again, "My AI wonâ€™t run..." ğŸ˜…
