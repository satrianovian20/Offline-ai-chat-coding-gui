# ğŸ§  Offline AI Chat & Coding GUI (Rakyat Edition)
GUI Python ringan (Tkinter) untuk menjalankan AI chat &amp; coding dengan model GGUF lokal (Low GUI PYTHON from Tkinter to run Offline AI Chat and Coding)

Moto:
GUI Rakyat Edition: Ringan, Garang, Tak Terbendung, AI GUI Bebas Crashâ„¢, Anti-Bluescreen Framework, The GUI That Refused to Die

ğŸš€ Cloned already? Donâ€™t forget to â­ if this project made your CPU smile ğŸ˜„  "ğŸ§ª Verified Stress Test Logs (Real-World AI Usage by Rakyat CPU)" ğŸ† "LLaMA.CPP Rakyat Edition: ChatGPT Experience in kilobytes." ğŸ’» "Stress-Tested Terminator AI GUI" ğŸš§ "Born in CPU Hell, Forged by Low-End Desperation"

---

ğŸ”¥ Tentang GUI Ini
GUI ini bukan lahir dari ketenangan.
Ia ditempa di CPU 100%, RAM nyaris kolaps, dan prompt berulang-ulang yang tak masuk akal.

Dari pengalaman hampir ngehang di v1, GUI ini berkembang jadi sistem tangguh yang mampu menjalankan model 13B, 15B, 16B, dan 17B di sistem lokal 16GB ram tanpa ampun.

Ini GUI untuk user yang tidak takut panas.

Didesain oleh pejuang offline, untuk pejuang offline.

â€œKalau dia bisa bertahan di neraka CPU saya, dia akan bertahan di neraka siapa pun.â€ â€” Developer

ğŸ”¥ GUI ini diuji oleh Battletester Elegan Pro Rakyat ğŸ”¥  
Memberi akses AI lokal ke semua pengguna, tanpa perlu GPU mahal  
Tidak perlu server, tidak perlu 32GB RAM, cukup niat dan logika  

â€œDiuji pertama oleh Battletester Elegan Pro Rakyat demi GUI yang kuat dan stabil.â€

ğŸ‘ï¸â€ğŸ—¨ï¸ Tujuan:  
Melindungi user dari GUI tidak stabil, menjaga kestabilan RAM, CPU, dan nyawa Windows Explorer Anda.

---

> ğŸ‡®ğŸ‡© This project is primarily documented in Indonesian.
> ğŸ‡¬ğŸ‡§ English overview is provided below.
> â€œThis project is based on the original LLaMA GUI by Satria Novian â€“ [GitHub link], licensed under the MIT License. Copyright Â© 2025 Satria Novian.â€ 

ğŸŒŸ Dokumentasi Fenomena Aneh GUI Rakyat Edition (Documentation of Strange Phenomena GUI Rakyat Edition)

| Bahasa Indonesia                                                                 | English                                                                              |
|----------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|
| ğŸ§ª Dicoba pakai model 17B Q5_K_M, tapi GUI-nya menolak mengaku overload RAM.     | ğŸ§ª Tried with 17B Q5_K_M model, but the GUI refused to say â€œRAM overloadâ€.           |
| ğŸ”€ Dicoba multitasking AI + Coding bareng di RAM 16 GB, tetap tak mau hang, crash, atau not responding. | ğŸ”€ Tried multitasking AI + Coding on 16 GB RAM, still refused to hang, crash, or freeze. |
| ğŸ§  GUI-nya kayak punya logika hidup sendiri, kayak ngomong: "Gua bukan GUI yang gampang nyerah." | ğŸ§  The GUI feels like it has a life of its own, saying: "I'm not the GUI that gives up." |
| ğŸ‘» Bahkan OS-nya kayak bingung: â€œLho, kenapa nggak bluescreen padahal beban udah kayak mau pecah?â€ | ğŸ‘» Even the OS seems confused: "Why no bluescreen when the load is clearly breaking point?" |
| ğŸ˜­ Crash, hang, not responding, dan bluescreen sekarang cuma bisa nangis di pojokan Task Manager. | ğŸ˜­ Crash, hang, unresponsive, and bluescreen now can only cry in the corner of Task Manager. |

ğŸ‘‰ TaskManager.exterminate(Bluescreen).sendTo("void\\system32")

"Error Detected.
Sending to: C:\Windows\System32\Void\NeverReturn.dll"
ğŸ’€ "May the bugs rest in bits."
ğŸ”¥ The GUI That Bans BSOD Itself.
ğŸ‘ Respect.
âœ… Certified Not Manja â€” Rakyat Mode Approved

ğŸ‡®ğŸ‡© Bahasa Indonesia:

ğŸ‰ Pertama kalinya dalam sejarah: Model AI 17B (Q5_K_M) berhasil dijalankan hanya dengan RAM 16GB â€” tanpa GPU, tanpa web server, cukup dengan GUI Python ukuran 10KB! Bukti nyata efisiensi maksimal ğŸ’ª
âš ï¸ Catatan RAM 16GB:

Meskipun sistem Anda 16GB, Windows dan Office 2024 bisa menyita 3â€“5 GB di background. Namun, GUI ini tetap dapat menjalankan model 13B Q4_K_M - 17B Q5_K_M. Tanpa GPU, tanpa swap besar.

âœ… GUI Python KB Ini langsung-to-the-point, tanpa ribet:
Prompt masuk â†’ Model jalan â†’ Output tampil

## âš–ï¸ Perbandingan GUI Python kb vs WebUI Berat

| **Fitur / Aspek**                | ğŸ **GUI Python KB (tkinter)** | ğŸŒ **WebUI (Gradio/Oobabooga dll.)** |
|----------------------------------|----------------------------------|--------------------------------------|
| âœ… Ukuran File GUI               | **10 KB**                        | > **100 MB**                         |
| âš™ï¸ Bahasa Pemrograman            | Python (tkinter native)          | Python + Gradio + JS + HTML          |
| ğŸ§  Model yang Diuji              | 1,3B (Q4_0) - 17B (Q5_K_M)                 | 7B, 13B                               |
| ğŸ§® RAM Minimum (8B (Q8_0) - 17B Q5_K_M)      | **12.3 â€“ 15.5 GB**               | **> 18 â€“ 20 GB**                      |
| ğŸ–¥ï¸ CPU Pengujian                | i5-9400F (no GPU)                | Biasanya pakai GPU / CPU kuat        |
| ğŸš€ Waktu Load Model 13B         | **35 â€“ 40 detik**                | Bisa > 1 menit                       |
| ğŸŸ¢ RAM Saat Idle                | 12.1 â€“ 12.4 GB                   | > 15 GB                               |
| ğŸ› ï¸ Konfigurasi Awal            | Hanya 1 file `.py`               | Banyak dependensi dan setup venv     |
| ğŸ“‰ Risiko Error/Crash           | **Sangat rendah / stabil**       | Kadang freeze / error token          |
| ğŸª„ Token yang Diuji Lancar      | 5000 token                | Bergantung setting/model             |
| ğŸ“¡ Server Web                   | **Tidak perlu**                  | Wajib buka server (http/websocket)   |
| ğŸ§© Dukungan Plugin              | Manual (custom)                  | Banyak tapi berat                    |
| ğŸ’¬ Chat & Coding Mode           | âœ… Sangat cocok                  | âœ… Cocok, tapi lebih berat            |
| ğŸ¤¯ Respons Not Responding?     | Hanya saat proses berat dan tanpa crash         | Sering delay jika RAM kritis         |

# Model yang sudah diuji

| No | Nama Model GGUF                                      | Ukuran Quant   | RAM yang Digunakan          | OS & Kondisi Tambahan                         | Status GUI       |
|----|------------------------------------------------------|--------------|---------------------|----------------------------------------------------|------------------|
| 1  | luna-ai-llama2-uncensored.Q4_0.gguf                 | Q4_0         | Â±11.6 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + music & anime video     | Stable & Smooth  |
| 2  | Meta-Llama-3-8B-Instruct.Q8_0.gguf                  | Q8_0         | Â±10.8 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + music & anime video     | Stable & Smooth  |
| 3  | WizardLM-13B-Uncensored.Q5_K_M.gguf                           | Q5_K_M       | 12,6 GB of 16 GB       |  Windows 11 pro 24H2 + Office 2024 | Tested + Stable & Smooth           |
| 4  | wizardcoder-python-13b-v1.0.Q5_K_M.gguf                           | Q5_K_M       | 12,6 GB of 16 GB       |  Windows 11 pro 24H2 + Office 2024 | Tested + Stable & Smooth           |
| 5  | All 7B           | Q4_K_M       | â‰¤11.5 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + Chrome + music video 720p                          | Stable & Smooth  |
| 6  | All 13B (Kecuali Yi 13B)           | Q4_K_M       | â‰¤15.5 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + Chrome + music video 720p                          | Stable & Smooth  |
| 7  | deepseek-coder-7b-instruct-v1.5-Q8_0.gguf           | Q8_0       | 10.8 GB of 16GB    | Windows 11 pro 24H2 + Office 2024                           | Stable & Smooth  |
| 8  | deepseek-coder-1.3b-instruct.Q4_0.gguf           | Q4_0       | 4 GB of 16GB    | Windows 11 pro 24H2 + Office 2024                           | Stable & Smooth  |
| 9  | codellama-13b.Q6_K.gguf           | Q6_K       | â‰¤15.4 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + Chrome + Notepad++                          | Stable & Smooth  |
| 10  | starcoder2-15b-Q5_K_M (1).gguf           | Q5_K_M       | â‰¤15.5 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + Chrome + Notepad++                          | Stable & Smooth  |
| 11  | DeepSeek-Coder-V2-Lite-Instruct-Q5_K_M.gguf           | Q5_K_M       | â‰¤15.5 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + Chrome + Notepad++                          | Stable & Smooth  |
| 12  | Llama-3-16B.Q5_K_M.gguf           | Q5_K_M       | â‰¤15.5 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + Chrome + Notepad++                          | Stable & Smooth  |
| 13  | orcamaidxl-17b-32k.Q5_K_M.gguf           | Q5_K_M       | â‰¤15.5 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + Chrome                          | Stable & Smooth  |

# ğŸ† GUI Performa Tinggi Berbasis Anomali (Edisi 2025)

| Peringkat | Nama Aplikasi              | Kategori Khusus                             | Framework       | Keanehan Utama                                                                      |
|----------|-----------------------------|---------------------------------------------|-----------------|--------------------------------------------------------------------------------------|
| ğŸ¥‡ #1    | multichat_ai_gui.py         | GUI Multichat AI Offline (Tkinter)          | Python Tkinter  | Jalan tanpa Flask/Gradio, tab AI paralel stabil, load GGUF langsung, resource-aware |
| ğŸ¥ˆ #2    | AutoGPT CLI                 | Agen AI Interaktif via Terminal             | Python CLI      | Task interaktif tanpa GUI, tetap terasa seperti multitasking                         |
| ğŸ¥‰ #3    | KoboldAI Lite GUI           | GUI Ringan untuk LLM                        | Tkinter/WebUI   | Jalankan LLM besar di hardware kentang tanpa crash GUI                              |
| ğŸ… #4    | A1111 Stable Diffusion WebUI| GUI Lanjutan untuk Gambar AI                | Gradio          | Bisa override batasan VRAM, ekstensinya tetap stabil                                 |

â€œTidak seperti banyak GUI AI lain, proyek ini didokumentasikan jujur dan lengkap, termasuk stress test ekstrem untuk rakyat yang ingin tahu kemampuan sesungguhnya.â€

ğŸ‡¬ğŸ‡§ English Version:

ğŸ‰ First time in history: A 17B (Q5_K_M) AI model runs smoothly with just 16GB RAM â€” no GPU, no web server, only a 6KB - 16kb Python GUI! Proof of ultimate efficiency ğŸ’ª

âš ï¸ 16GB RAM Note:

Even if your system has 16GB, Windows and Office 2024 can take up 3â€“5GB in the background. However, this GUI can still run the 13B Q4_K_M - 17B Q5_K_M model efficiently. No GPU, no large swap requirements.

âœ… This Python KB GUI is straight-to-the-point, without any fuss:
Enter prompt â†’ Run model â†’ Display output

## âš–ï¸ Comparison: Python GUI (KB) vs Heavy WebUIs

| **Feature / Aspect**             | ğŸ **Python GUI KB (Tkinter)**       | ğŸŒ **WebUIs (Gradio/Oobabooga, etc.)**     |
|:----------------------------------|:----------------------------------------|:--------------------------------------------|
| âœ… **GUI File Size**              | **10 KB**                               | > **100 MB**                                 |
| âš™ï¸ **Programming Language**       | Native Python (Tkinter)                 | Python + Gradio + JS + HTML                  |
| ğŸ§  **Tested Model Types**         | 1,3B (Q4_0) - 17B (Q5_K_M)                        | 7B, 13B                                       |
| ğŸ§® **Minimum RAM (8B Q8_0 â€“ 17B Q5_K_M)**| **12.3 â€“ 15.5 GB**                      | **> 18 â€“ 20 GB**                              |
| ğŸ–¥ï¸ **Test CPU**                   | i5-9400F (no GPU)                       | Typically uses GPU or high-end CPU           |
| ğŸš€ **Model Load Time (13B)**      | **35 â€“ 40 seconds**                     | Can take > 1 minute                          |
| ğŸŸ¢ **Idle RAM Usage**             | 12.1 â€“ 12.4 GB                          | > 15 GB                                       |
| ğŸ› ï¸ **Initial Setup**             | Single `.py` file only                  | Many dependencies and venv setup             |
| ğŸ“‰ **Error / Crash Risk**         | **Very low / stable**                   | Sometimes freezes or token errors            |
| ğŸª„ **Smooth Token Output Tested** | 5000 tokens                             | Varies by settings/model                     |
| ğŸ“¡ **Web Server Required**        | **Not needed**                          | Required (HTTP/websocket server)             |
| ğŸ§© **Plugin Support**             | Manual (customizable)                   | Many, but resource-heavy                     |
| ğŸ’¬ **Chat & Coding Mode**         | âœ… Highly suitable                      | âœ… Suitable, but heavier                      |
| ğŸ¤¯ **"Not Responding" Behavior**  | Only during heavy loads, no crash       | Frequent delays when RAM is low              |

# Tested Model

| No | GGUF Model Name                                      | Quant Type   | RAM Usage          | OS & Additional Conditions                         | GUI Status       |
|----|------------------------------------------------------|--------------|---------------------|----------------------------------------------------|------------------|
| 1  | luna-ai-llama2-uncensored.Q4_0.gguf                 | Q4_0         | Â±11.6 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + music & anime video     | Stable & Smooth  |
| 2  | Meta-Llama-3-8B-Instruct.Q8_0.gguf                  | Q8_0         | Â±10.8 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + music & anime video     | Stable & Smooth  |
| 3  | WizardLM-13B-Uncensored.Q5_K_M.gguf                           | Q5_K_M       | 12,6 GB of 16 GB       |  Windows 11 pro 24H2 + Office 2024 | Tested + Stable & Smooth           |
| 4  | wizardcoder-python-13b-v1.0.Q5_K_M.gguf                           | Q5_K_M       | 12,6 GB of 16 GB       |  Windows 11 pro 24H2 + Office 2024 | Tested + Stable & Smooth           |
| 5  | All 7B           | Q4_K_M       | â‰¤11.5 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + Chrome + music video 720p                          | Stable & Smooth  |
| 6  | All 13B (except Yi 13B)           | Q4_K_M       | â‰¤15.5 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + Chrome + music video 720p                           | Stable & Smooth  |
| 7  | deepseek-coder-7b-instruct-v1.5-Q8_0.gguf           | Q8_0       | 10.8 GB of 16GB    | Windows 11 pro 24H2 + Office 2024                           | Stable & Smooth  |
| 8  | deepseek-coder-1.3b-instruct.Q4_0.gguf           | Q4_0       | 4 GB of 16GB    | Windows 11 pro 24H2 + Office 2024                           | Stable & Smooth  |
| 9  | codellama-13b.Q6_K.gguf           | Q6_K       | â‰¤15.4 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + Chrome + Notepad++                          | Stable & Smooth  |
| 10  | starcoder2-15b-Q5_K_M (1).gguf           | Q5_K_M       | â‰¤15.5 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + Chrome + Notepad++                          | Stable & Smooth  |
| 11  | DeepSeek-Coder-V2-Lite-Instruct-Q5_K_M.gguf           | Q5_K_M       | â‰¤15.5 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + Chrome + Notepad++                          | Stable & Smooth  |
| 12  | Llama-3-16B.Q5_K_M.gguf           | Q5_K_M       | â‰¤15.5 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + Chrome + Notepad++                          | Stable & Smooth  |
| 13  | orcamaidxl-17b-32k.Q5_K_M.gguf           | Q5_K_M       | â‰¤15.5 GB of 16GB    | Windows 11 pro 24H2 + Office 2024 + Chrome                          | Stable & Smooth  |


# ğŸ† Anomaly-Based Performant GUI Hall of Fame (2025 Edition)

| Rank     | Application Name            | Special Category                            | Framework        | Key Anomaly                                                                        |
|----------|-----------------------------|----------------------------------------------|------------------|------------------------------------------------------------------------------------|
| ğŸ¥‡ #1    | multichat_ai_gui.py         | Offline Multichat AI GUI (Tkinter)           | Python Tkinter   | No Flask/Gradio, stable multitabs, GGUF loader, resource-aware and unexplained smoothness |
| ğŸ¥ˆ #2    | AutoGPT CLI                 | Interactive Terminal AI Agent                 | Python CLI       | Complex task agent in terminal, acts like a GUI                                    |
| ğŸ¥‰ #3    | KoboldAI Lite GUI           | Lightweight LLM GUI                           | Tkinter/WebUI    | Runs large LLMs on low-end systems, GUI doesnâ€™t crash                              |
| ğŸ… #4    | A1111 Stable Diffusion WebUI| Advanced AI Image GUI                         | Gradio           | Can override VRAM limits, extensions run stably                                    |

â€œUnlike many other AI GUIs, this project is documented transparently and thoroughly â€” including extreme stress tests â€” for the people who want to know the real capabilities of their hardware and local models.â€

## ğŸ“¸ Screenshot GUI From llamacpp_multichat_ai_gui.py
![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-27%20082259.png)

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-28%20174118.png)

## ğŸ“¸ Screenshot GUI From llamacpp_gui_combinedv11.py

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-23%20073915.png)

## ğŸ“¸ Screenshot GUI From llamacpp_gui_combinedv10.py

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/Screenshot%202025-07-20%20100422.png)

## ğŸ“¸ Screenshot GUI From llamacpp_gui_combinedv9.py

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-18%20160408.png)

## ğŸ“¸ Screenshot GUI From llamacpp_gui_mode.py (GUI Experiment)

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-19%20110119.png)

## ğŸ“¸ Screenshot Chat From GUI
![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-18%20170753.png)

## ğŸ“¸ Second Screenshot Chat From GUI
![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-18%20170809.png)

## ğŸ“¸ Third Screenshot Chat From GUI
![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-30%20064550.png)

---

Bahasa Indonesia
# ğŸ”Œ LLaMA.cpp Port Detector GUI

Alat sederhana GUI untuk mendeteksi port aktif dari `llama-server.exe` pada LLaMA.cpp build lokal Anda.

## ğŸ“¦ Fitur
- Deteksi otomatis port aktif dari range yang ditentukan
- Tes koneksi ke port manual
- Salin port ke clipboard
- Ubah hostname (127.0.0.1 atau localhost)

## ğŸ–¥ï¸ Cara Pakai
1. Jalankan `_py test.cmd` terlebih dahulu.
2. `LLaMA_Server_Port_Detector.py` akan langsung jalan setelah Jalankan `_py test.cmd` terlebih dahulu.
3. Klik 'Jalankan llama-server.exe'.
4. Klik `ğŸ” Deteksi Port`.
5. Setelah port aktif ditemukan, bisa langsung klik `ğŸ“‹ Copy Port` atau `ğŸ”Œ Tes Koneksi`.

## ğŸ’¡ Catatan
- Script ini menggunakan `tkinter` dan `http.client`.
- Tidak perlu install `pyperclip` jika tidak bisa, bisa dihapus fitur `Copy`.

### ğŸ“„ `requirements.txt` (opsional)
# Hanya jika pyperclip dipakai
pyperclip

---

English Version

ğŸ”Œ LLaMA.cpp Port Detector GUI
A simple GUI tool to detect the active port used by llama-server.exe on your local LLaMA.cpp build.

ğŸ“¦ Features
Automatically scans for active ports within a specified range

Manually test connection to a specific port

Copy the detected port to clipboard

Change hostname (127.0.0.1 or localhost)

ğŸ–¥ï¸ How to Use
1. Run _py test.cmd first.

2. Will Launch LLaMA_Server_Port_Detector.py after Run _py test.cmd first.

3. Click 'Jalankan llama-server.exe'.

4. Click ğŸ” Detect Port.

5. Once an active port is found, you can click ğŸ“‹ Copy Port or ğŸ”Œ Test Connection.

ğŸ’¡ Notes
This script uses tkinter and http.client.

Installing pyperclip is optional; if not available, the copy feature can be disabled.

ğŸ“„ requirements.txt (optional)
# Only needed if you use pyperclip
pyperclip

---

## ğŸ‡®ğŸ‡© Bahasa Indonesia

GUI lokal untuk LLaMA.cpp:

ğŸ“ Cocok untuk chat dan coding!

ğŸ’¬ Contoh jawaban untuk user awam:
â€œGUI ini nggak ngebatesin kemampuan AI-nya. Kamu bisa atur output token sesuai kemampuan RAM PC kamu. Misal RAM kamu cuma 8GB, kamu bisa atur max token ke 150 biar tetap lancar. Kalau RAM kamu 16GB, bisa gaspol sampai 10.000 token. Jadi fleksibel, ringan, dan tetap powerful!â€

## ğŸ“¸ Screenshot (Model berhasil dimuat di RAM 16GB)

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-17%20114247.png)

## ğŸ“¸ Screenshot (Model berhasil dimuat di RAM 16GB)

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-17%20125025.png)

## ğŸ“¸ Screenshot (ğŸ§  Stress Tested di RAM 16GB)
[ğŸ§  Stress Tested](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/screenshot5.jpg)

## ğŸ“¸ Screenshot (ğŸ§  Stress Tested di RAM 16GB)
[ğŸ§  Stress Tested](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/screenshot4.jpg)

ğŸ“¸ Bukti: Sudah dilampirkan screenshot dan log lengkap di repo

---

ğŸ’¡ Kesimpulan Buat Pengguna GUI:
Kalau user punya RAM:
4GB: 1,3B Q8_0, 1,5B Q8_0, 4B Q4_K_M, dan 4B Q5_K_M
Ctx Size: 1024
Maks Token: 150

8GB: 1,3B Q8_0, 1,5B Q8_0, 4B Q8_0, 7B Q4_0, dan 7B Q5_K_M
Ctx Size: 2048
Maks Token: 512

12GB:1,3B Q8_0, 1,5B Q8_0, 4B Q8_0, 7B Q8_0, dan 8B Q5_K_M
Ctx Size: 4096
Maks Token: 1000

16GB: 1,3B Q8_0, 1,5B Q8_0, 4B Q8_0, 7B Q8_0, 8B Q8_0, 9B Q8_0, 10,7B Q8_0, 13B Q6_k, 15B Q5_K_M, 16B Q5_K_M, dan 17B Q5_K_M
Ctx Size: 100k 
Maks Token: 10k

---

Optimalitas GUI Python 6KB-16kb:
âœ… GUI Python Terbukti Optimal dan Stabil

GUI ini hanya berukuran 10KB namun sudah terbukti mampu menangani model AI lokal besar seperti 7B dan 13B tanpa eror, crash, atau freeze, bahkan saat dijalankan di PC tanpa GPU.

ğŸ“Œ Keunggulan:

âœ”ï¸ Ukuran super ringan (10KB)

âœ”ï¸ Bisa langsung jalan tanpa instalasi ribet

âœ”ï¸ Menggunakan llama.cpp sebagai backend (kompatibel GGUF)

âœ”ï¸ RAM usage stabil dan ada optimalisasi

âœ”ï¸ Load model 13B sukses tanpa error

âœ”ï¸ GUI tetap responsif dan tidak berat

âœ”ï¸ Output respons tetap stabil meski model besar

ğŸ’¡ GUI ini bahkan lebih optimal dibanding banyak UI besar di luar sana karena tidak menyia-nyiakan resource, cocok untuk pengguna low-end PC yang tetap ingin merasakan kekuatan model besar lokal.

---

ğŸ“Œ Bonus Penjelasan Visual (Flowchart Saran)

[Mulai GUI]
   â†“
[Load Model]
   â†’ RAM = 15.5GB
   â†’ CPU 60%
   â†“
[Model Idle]
   â†’ RAM = 12.3GB
   â†’ CPU 5%
   â†“
[Prompt Masuk]
   â†’ CPU 100%
   â†“
[Generate Response]
   â†’ Output OK
   â†“
[Idle Lagi]
   â†’ RAM tetap
   â†’ CPU turun

---

## ğŸ§  Alur Penanganan Prompt di GUI Multichat AI

Berikut adalah flowchart proses alur kerja prompt untuk multitab AI chat offline berbasis GUI Python:

flowchart TD
[User] / [Pengguna]
      â†“
Click 'Start Model' / Klik 'Mulai Model'
      â†“
Model Loaded in Tab Utama / Model Dimuat di Tab Utama
      â†“
User opens New Chat Tab / Pengguna membuka Tab Chat Baru
      â†“
Send Prompt (Queued) / Kirim Prompt (Antri)
      â†“
Prompt processed sequentially / Prompt diproses secara berurutan
      â†“
AI generates response / AI menghasilkan respons
      â†“
Display response in correct tab / Tampilkan respons di tab yang sesuai

ğŸ”„ Penjelasan Singkat:
Prompt dijalankan satu per satu berdasarkan waktu dikirimnya.

Semua tab chat hanya aktif jika model sudah dimuat dari Tab Utama.

Tidak ada campur jawaban karena buffer tiap tab terpisah dan memakai antrian per thread.

---

âœ… Recap Status Dokumentasi (versi Indonesia):

| Elemen Bukti |	Status  |
|------|-----|
| ğŸ“œ Log sesi (session-logs) |	âœ… Sudah ada |
| ğŸ–¼ï¸ Screenshot saat load model	| âœ… Sudah ada |
| ğŸ–¼ï¸ Screenshot saat Idle |	âœ… Sudah ada|
| ğŸ§ª Model besar (13B Q5_K_M)	| âœ… Sudah diuji |
|âš™ï¸ GUI ringan (6KB-16kb Python script) |	âœ… Terpakai dengan lancar dan tanpa crash karena adanya fitur otomatis yang melakukan optimalisasi ram dan cpu |

---

âœ… FAQ â€” Pertanyaan Umum (Trust Booster Edition)
â“ GUI ini beneran bisa jalanin model 13B tanpa GPU?
Ya! Sudah diuji langsung dengan model llama-2-13b-chat.Q4_K_M.gguf di sistem dengan:

ğŸ’» CPU: Intel i5-9400F (tanpa iGPU)

ğŸ§  RAM: 16GB DDR4

âš™ï¸ Backend: llama.cpp

ğŸ“¦ GUI: Python script ukuran hanya 10KB

â“ Bukti nyatanya mana?
ğŸ“¸ Screenshot saat load model dan idle sudah diunggah di folder docs/screenshots/

ğŸ“„ Log lengkap sesi percobaan model 13B tersedia di docs/session-logs/

Tidak ada error, tidak crash, hanya delay wajar saat proses berat.

â“ GUI-nya berat gak?
Tidak. GUI ini hanya 10KB, tanpa dependensi besar seperti Gradio atau Electron.

Tidak buka port aneh-aneh.

Tidak ada tracking.

Murni offline dan lokal.

UI sangat ringan, hanya berbasis tkinter.

â“ Bisa pakai model 7B, 8B, atau 13B lain?
Bisa! Sudah diuji dengan:

Mistral 7B

DeepSeek Coder 6.7B

DeepSeek Coder 7B

Nous Hermes 13B (Q4_K_M)

LLaMA 13B (Q4_K_M)

â“ RAM saya cuma 8GB, bisa jalan?
Bisa, asal model yang dipilih sesuai. Gunakan model kecil seperti:

TinyLlama 1.1B Q8_0 

DeepSeek Coder 13b Q8_0 

Mistral 4B Q8_0 

Open Hermes 7B Q4_K_M

Atur max_tokens di GUI agar tidak melebihi kapasitas RAM kamu.

â“â€œSaya masih nggak percaya GUI ini bisa jalanin model 13B cuma dengan RAM 16GB. Beneran bisa?â€
ğŸ’¬ â€œCoba sendiri aja bro, repo udah public kok ğŸ˜â€

â“â€œEmang GUI-nya ringan banget ya?â€
âœ… Iya. Ukuran file .py cuma 10KB. Gak ada embel-embel web server, backend rumit, atau library berat.

â“â€œBisa crash gak pas load model besar?â€
ğŸš« Selama sistem kamu stabil dan swap file aktif, hampir nggak pernah crash. Bahkan log menunjukkan performa tetap normal walau RAM di atas 15GB pas awal load.

â“â€œAda buktinya?â€
ğŸ“¸ Sudah ada screenshot dan log di folder docs/session-logs/ dan docs/screenshots/.

â“â€œKalau saya nggak percaya tetap?â€
ğŸ˜ Silakan buktikan sendiri. Semuanya open source. Mau test sendiri? Silakan kloning repo-nya sekarang.

Catatan Jujur (untuk README atau FAQ)
ğŸ’¡ Saat ini, pengujian terbatas pada model hingga 15B (Q5_K_M) karena sistem hanya memiliki RAM 16GB tanpa GPU.
Namun, GUI Python 6KB-16KB ini berhasil menangani model besar tersebut secara stabil dan efisien, yang biasanya tidak mungkin dilakukan tanpa sistem high-end.
Untuk model 33B ke atas, uji coba akan dilakukan jika tersedia perangkat dengan RAM lebih besar. Ditambah bisa melakukan optimalisasi ram dan cpu.

---

## ğŸ“¦ Daftar Versi GUI

Berikut versi-versi GUI yang berhasil diuji:

| Versi File                 | Status  | Fitur Utama                             |
|---------------------------|---------|------------------------------------------|
| llamacpp_gui_combined.py  | âœ… Stabil | Versi awal gabungan GUI chat + sistem    |
| llamacpp_gui_combinedv2.py| âœ…        | Tambahan pengaturan model & prompt       |
| llamacpp_gui_combinedv3.py| âœ…        | Fix error kecil, UI lebih bersih         |
| llamacpp_gui_combinedv4.py| âœ…        | + Logging dan auto-load model            |
| llamacpp_gui_combinedv5.py| âœ…        | + Riwayat chat dan sistem prompt         |
| llamacpp_gui_combinedv6.py| âœ…        | + Theme mode dan pengaturan lanjutan     |
| llamacpp_gui_combinedv7.py| âœ…        | + Auto-save session dan repeat_penalty   |
| llamacpp_gui_combinedv8.py| âœ…       | + Auto-save load model berfungsi        |
| llamacpp_gui_combinedv9.py| âœ…        | + Fitur Rakyat Mode di menu pengaturan dan prompt hemat ram di samping mulai Llama Server        |
| llamacpp_gui_combinedv10.py    | âœ…               | + Generate Prompt Words per chunk, chunk delay, auto chunk resume, auto chunkresum via prompt, chat context, save context, dan load save context       |
| llamacpp_gui_combinedv11.py    | âœ… Terbaru              | + Memory Mode dan Versi Terakhir       |
| llamacpp_multichat_ai_gui.py    | âœ… Terbaru              | + Memory Mode + Multi Chat Tab AI + Generate Chunk + Chat Context + Auto Save Chat Context + Auto Save Chat History + Import Prompt From File + Close Tab + Prompt System + CTX Size and  Tokens Settings + Change Model + Reset Chat Display      |
| llamacpp_rakyatmode.py| âœ… Terbaru | + --ctx-size default ke 1024 + Lowram        |
| llamacpp_gui_mode.py      | âœ… Eksperimen + Stabil | Mode GUI ringan eksperimen + load model 13 Q4_K_M               |
| llamacpp_gui_modev2.py    | âœ…        | Kombinasi GUI mode dengan layout sistem  |

## ğŸ’¡ Syarat Minimum PC

| Komponen         | Minimum                    |
|------------------|-----------------------------|
| Prosesor         | i3/i5 Gen 8+ (atau Ryzen 3+) |
| RAM              | 8-16 GB (direkomendasikan 32 GB - 64 gb)|
| GPU              | GTX 1650 / setara (VRAM 4GB) |
| OS               | Windows 10/11 64-bit        |
| Python           | 3.10+                       |

---

ğŸ¤ Kontribusi & Kredit
ğŸ‘¨â€ğŸ’» Creator: [satrianovian20] â€“ Modifikasi GUI offline dengan Tkinter

âœï¸ Kontributor AI Script & Fixer Error: ChatGPT

ğŸ” Model by Meta (LLaMA), WizardLM, dan komunitas open-source

ğŸ’¡ Terinspirasi oleh kesulitan real pengguna dengan PC low-end

---

## ğŸ“¦ Cara Install & Jalankan

### 1. Download Python
Install Python 3.10 dari https://www.python.org/downloads/release/python-3100/

- âœ… Centang â€œAdd Python to PATHâ€ saat install!
- pip install requests
- download dari link: (https://github.com/ggml-org/llama.cpp/releases) [llama-b59xx-bin-win-cpu-x64 atau llama-b5899-bin-win-cpu-x64]
- letakkan gui python di folder dari llamacpp yang sudah di download dari link: (https://github.com/ggml-org/llama.cpp/releases)
- Jalankan gui python dengan klik dua kali
> Catatan: cari saja yang berfungsi dari build llama cpp untuk muat model ai di link: (https://github.com/ggml-org/llama.cpp/releases). Biasanya yang berfungsi antara yang terbaru yaitu llama-b59xx-bin-win-cpu-x64 atau llama-b5899-bin-win-cpu-x64. Intinya fitur muat model di gui sepenuhnya berfungsi tapi llamacpp sebagai backend kurang berfungsi dengan baik sehingga silahkan cari llama-b5899-bin-win-cpu-x64 - llama-b59xx-bin-win-cpu-x64 yang dapat muat model.

---

## ğŸ’– Dukung Proyek Ini

Jika kamu merasa proyek ini bermanfaat dan ingin mendukung pengembangannya, kamu bisa berdonasi melalui:

- ğŸ’¸ [Saweria](https://saweria.co/satrianovian20)
- â˜• [PayPal](https://www.paypal.com/paypalme/satrianovian)

Terima kasih banyak atas dukungannya! ğŸ™

## 2. Clone atau Download Proyek Ini

git clone https://github.com/satrianovian20/offline-ai-chat-coding-gui.git
cd offline-ai-chat-coding-gui

## â˜• Penutup
Proyek ini dibuat karena keterbatasan memunculkan inovasi. Saya juga pernah pusing karena WebUI dari GitHub gagal jalan, sampai akhirnya saya:

Buat GUI sendiri

Cuma pakai 1 file Python untuk AI Chat & Coding

Bisa sambil nonton YouTube dan nonton video musik 720p ğŸ˜„

## ğŸ’¬ Kalau kamu juga merasakan manfaatnya, bantu bintang repo ini!
Supaya tidak ada lagi yang pusing karena â€œAI-nya gak bisa jalan...â€ ğŸ˜…

---

## English (International)

âš™ï¸ Local GUI for LLaMA.cpp
ğŸ“ Suitable for Chat and Coding!

---

ğŸ’¬ Example explanation for beginners:
â€œThis GUI doesnâ€™t limit your AIâ€™s capabilities. You can adjust the output token count based on your PCâ€™s RAM. For example, if you only have 8GB RAM, set the max tokens to 150 for smooth performance. If you have 16GB RAM, you can push it up to 10,000 tokens. Itâ€™s flexible, lightweight, and still powerful!â€

---

ğŸ“¸ Screenshot (Model loaded successfully on 16GB RAM)

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-17%20114247.png)

ğŸ“¸ Screenshot (Model loaded successfully on 16GB RAM)

![Model Loaded Screenshot](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/Screenshot%202025-07-17%20125025.png)

## ğŸ“¸ Screenshot (ğŸ§  Stress Tested in 16GB RAM)
[ğŸ§  Stress Tested][#stress-test-report](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/screenshot5.jpg)

## ğŸ“¸ Screenshot (ğŸ§  Stress Tested in 16GB RAM)
[ğŸ§  Stress Tested][#stress-test-report](https://github.com/satrianovian20/offline-ai-chat-coding-gui/blob/main/doc/screenshot4.jpg)

ğŸ“¸ Proof: Full screenshots and logs are available in the repo

âœ… Python GUI 6KBâ€“10KB Proven Stable and Optimal
This Python GUI script weighs just 10KB, yet has proven capable of running large local models like 7B and 13B without errors, crashes, or freezing â€” even on non-GPU machines.

---

ğŸ’¡ Conclusion for GUI Users:

If you have RAM:

4GB: 1,3B Q8_0, 1,5B Q8_0, 4B Q4_K_M, dan 4B Q5_K_M
Ctx Size: 1024
Max Token: 150

8GB: 1,3B Q8_0, 1,5B Q8_0, 4B Q8_0, 7B Q4_0, dan 7B Q5_K_M
Ctx Size: 2048
Max Token: 512

12GB:1,3B Q8_0, 1,5B Q8_0, 4B Q8_0, 7B Q8_0, dan 8B Q5_K_M
Ctx Size: 4096
Maks Token: 1000

16GB: 1,3B Q8_0, 1,5B Q8_0, 4B Q8_0, 7B Q8_0, 8B Q8_0, 9B Q8_0, 10,7B Q8_0, 13B Q6_k, 15B Q5_K_M, 16B Q5_K_M, dan 17B Q5_K_M
Ctx Size: 100k 
Max Token: 10k

---

ğŸ“Œ Highlights:
âœ”ï¸ Super lightweight (10KB)
âœ”ï¸ No complex installation
âœ”ï¸ Uses llama.cpp as backend (GGUF compatible)
âœ”ï¸ Stable RAM usage and RAM/CPU optimization built-in
âœ”ï¸ Successfully loads 13B models
âœ”ï¸ GUI remains responsive
âœ”ï¸ Output is stable even with large models

ğŸ’¡ This GUI is even more efficient than many bulky WebUIs because it doesnâ€™t waste system resources. Ideal for low-end PC users who want to harness the power of large local models.

---

ğŸ“Œ Visual Explanation (Flowchart Suggestion)
[Start GUI]
   â†“
[Load Model]
   â†’ RAM = 15.5GB
   â†’ CPU = 60%
   â†“
[Model Idle]
   â†’ RAM = 12.3GB
   â†’ CPU = 5%
   â†“
[Prompt Received]
   â†’ CPU = 100%
   â†“
[Generate Response]
   â†’ Output OK
   â†“
[Idle Again]
   â†’ RAM unchanged
   â†’ CPU drops

---

## ğŸ‡¬ğŸ‡§ Prompt Handling Flow (English Version)

## ğŸ§  Prompt Handling Flow in Multichat AI GUI

This flowchart illustrates how prompt handling works in the offline multitab AI chat GUI (Python-based):

[User] / [Pengguna]
      â†“
Click 'Start Model' / Klik 'Mulai Model'
      â†“
Model Loaded in Tab Utama / Model Dimuat di Tab Utama
      â†“
User opens New Chat Tab / Pengguna membuka Tab Chat Baru
      â†“
Send Prompt (Queued) / Kirim Prompt (Antri)
      â†“
Prompt processed sequentially / Prompt diproses secara berurutan
      â†“
AI generates response / AI menghasilkan respons
      â†“
Display response in correct tab / Tampilkan respons di tab yang sesuai

ğŸ”„ Quick Notes:
Prompts are handled one-by-one in the order they were sent.

All chat tabs are only enabled after the model is loaded from the main tab.

Each tab has its own buffer and runs via queue threads â€” preventing output mix-up.

---


âœ… Documentation Status Recap (English Version)

| Proof Element	| Status |
|----------------|---------|
| ğŸ“œ Session logs |	âœ… Available |
| ğŸ–¼ï¸ Screenshot (model load)  |	âœ… Available |
| ğŸ–¼ï¸ Screenshot (idle)	| âœ… Available |
| ğŸ§ª Large model (13B Q5_K_M)	 | âœ… Tested |
| âš™ï¸ Lightweight GUI (6KBâ€“16KB Python)	| âœ… Works smoothly and safely with RAM/CPU optimization built-in |

---

âœ… FAQ â€” Frequently Asked Questions (Trust Booster Edition)
â“ Can this GUI really run a 13B model without a GPU?
âœ… Yes! Successfully tested with llama-2-13b-chat.Q4_K_M.gguf on:

ğŸ’» CPU: Intel i5-9400F (no iGPU)

ğŸ§  RAM: 16GB DDR4

âš™ï¸ Backend: llama.cpp

ğŸ“¦ GUI: Only a 10KB Python script

â“ Whereâ€™s the real proof?
ğŸ“¸ Screenshots during model load and idle are uploaded to docs/screenshots/
ğŸ“„ Complete 13B model session logs available in docs/session-logs/
âœ… No errors, no crashes. Just slight delay under heavy processing â€” perfectly normal.

â“ Is this GUI heavy?
âŒ Not at all. Itâ€™s just 10KB. No bloated dependencies like Gradio or Electron.
âœ”ï¸ No random ports. No tracking.
âœ”ï¸ 100% offline and local.
âœ”ï¸ Based purely on Tkinter.

â“ Can I use other 7B, 8B, or 13B models?
âœ… Absolutely! Already tested with:

Mistral 7B

DeepSeek Coder 6.7B

DeepSeek Coder 7B

Nous Hermes 13B (Q4_K_M)

LLaMA 13B (Q4_K_M)

â“ I only have 8GB RAM, will it work?
âœ… Yes, just use smaller models like:

TinyLlama 1.1B Q8_0 

DeepSeek Coder 13b Q8_0 

Mistral 4B Q8_0 

Open Hermes 7B Q4_K_M

ğŸ› ï¸ Set max_tokens low to match your available RAM in the GUI settings.

â“ â€œI still donâ€™t believe this GUI can run 13B on just 16GB RAM. Really?â€
ğŸ’¬ â€œTry it yourself, bro. The repo is public ğŸ˜â€

â“ â€œIs the GUI really that lightweight?â€
âœ… Yep. File size is only 10KB.
No web servers, no complex backends, no heavy libraries.

â“ â€œWill it crash when loading large models?â€
ğŸš« As long as your system is stable and swap file is active, crashes are extremely rare.
ğŸ“Š Even with RAM above 15GB during model load, logs show stable performance.

â“ â€œIs there actual proof?â€
ğŸ“¸ Yes. Screenshots and logs are available in the docs/session-logs/ and docs/screenshots/ folders.

â“ â€œWhat if I still donâ€™t believe?â€
ğŸ˜ Feel free to test it yourself. Everything is open-source.
Clone the repo now and experience it.

ğŸ“œ Honest Note (for README or FAQ)
ğŸ’¡ So far, this GUI has only been tested with models up to 15B (Q5_K_M) due to 16GB RAM limitations and no GPU.
But the Python GUI (6KBâ€“16KB) still handled it smoothly and efficiently â€” something many wouldnâ€™t expect without a high-end setup.

For 33B+ models, testing will follow when more RAM is available. The GUI already supports CPU/RAM optimization.

---

## ğŸ“¦ GUI Version List

Below are the GUI versions that have been successfully tested:


| File Version                  | Status                 | Main Features                                                          |
|----------------------------------|----------------------------------|--------------------------------------|
| llamacpp_gui_combined.py      | âœ… Stable              | Initial combined version of chat + system GUI                          |
| llamacpp_gui_combinedv2.py    | âœ…                     | Added model & prompt configuration options                             |
| llamacpp_gui_combinedv3.py    | âœ…                     | Minor bug fixes, cleaner UI                                            |
| llamacpp_gui_combinedv4.py    | âœ…                     | + Logging support and auto-load model feature                          |
| llamacpp_gui_combinedv5.py    | âœ…                     | + Chat history and system prompt support                               |
| llamacpp_gui_combinedv6.py    | âœ…                     | + Theme mode and advanced configuration                                |
| llamacpp_gui_combinedv7.py    | âœ…                     | + Auto-save session and repeat_penalty setting                         |
| llamacpp_gui_combinedv8.py    | âœ…                     | + Auto-save model loading now works properly                           |
| llamacpp_gui_combinedv9.py    | âœ…               | + â€œRakyat Modeâ€ in settings + RAM-friendly prompt + LLaMA Server       |
| llamacpp_gui_combinedv10.py    | âœ…               | + Generate Prompt Words per chunk, chunk delay, auto chunk resume, auto chunkresum via prompt, chat context, save context, and load save context      |
| llamacpp_gui_combinedv11.py    | âœ… Latest              | + Memory Mode and Final Version      |
| llamacpp_multichat_ai_gui.py    | âœ… Latest              | + Memory Mode + Multi Chat Tab AI + Generate Chunk + Chat Context + Auto Save Chat Context + Auto Save Chat History + Import Prompt From File + Close Tab + Prompt System + CTX Size and  Tokens Settings  + Change Model + Reset Chat Display   |
| llamacpp_rakyatmode.py        | âœ… Latest              | + --ctx-size defaults to 1024 + optimized for low RAM                  |
| llamacpp_gui_mode.py          | âœ… Experimental/Stable | Lightweight GUI mode + supports loading 13B Q4_K_M model               |
| llamacpp_gui_modev2.py        | âœ…                     | Combination of GUI mode and system layout                              |

---

ğŸ’¡ Minimum PC Requirements

| Componentn         | Minimum Spec                    |
|------------------|-----------------------------|
| Prosesor         | i3/i5 Gen 8+ (or Ryzen 3+) |
| RAM              | 8-16 GB (32GBâ€“64GB recommended)|
| GPU              | GTX 1650 / equivalent (VRAM 4GB) |
| OS               | Windows 10/11 64-bit        |
| Python           | 3.10+                       |

---

ğŸ¤ Contributions & Credits
ğŸ‘¨â€ğŸ’» Creator: [satrianovian20] â€“ Modified offline GUI using Tkinter
âœï¸ AI Script & Error Fix Contributions: ChatGPT
ğŸ” Models by Meta (LLaMA), WizardLM, and the open-source community
ğŸ’¡ Inspired by real-world issues faced by low-end PC users

---

ğŸ“¦ How to Install & Run
1. Download Python
Get Python 3.10 from: https://www.python.org/downloads/release/python-3100/

- âœ… Check "Add Python to PATH" during installation!
- pip install requests
- download from the link: (https://github.com/ggml-org/llama.cpp/releases) [llama-b59xx-bin-win-cpu-x64 atau llama-b5899-bin-win-cpu-x64]
- place the python gui in the llamacpp folder downloaded from the link: (https://github.com/ggml-org/llama.cpp/releases)
- Run the python gui by double-clicking it
> Note: Just look for a working Llama CPP build to load AI models in link: (https://github.com/ggml-org/llama.cpp/releases). Typically, the latest ones are llama-b59xx-bin-win-cpu-x64 or llama-b5899-bin-win-cpu-x64. The point is that the model loading feature in the GUI is fully functional but llamacpp as a backend is not functioning properly so please look for llama-b5899-bin-win-cpu-x64 - llama-b59xx-bin-win-cpu-x64 which can load the model.

---

ğŸ’– Support This Project
If this project helped you and you'd like to support its development, you can donate via:

- ğŸ’¸ [Saweria](https://saweria.co/satrianovian20)
- â˜• [PayPal](https://www.paypal.com/paypalme/satrianovian)

Thank you so much for your support! ğŸ™

---

â˜• Final Note
This project was born from limitation and turned into innovation.
Like you, I struggled with WebUIs from GitHub that just wouldnâ€™t run.
So I:

Built my own GUI

With just a single .py file

For AI Chat & Coding

While still able to watch YouTube or music videos in 720p ğŸ˜„

ğŸ’¬ If this helped you, consider starring the repo!
Letâ€™s make sure no one ever has to say â€œMy AI wonâ€™t start...â€ again ğŸ˜…
