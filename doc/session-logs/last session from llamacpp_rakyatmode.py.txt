💬 Selamat datang di Rakyat Mode![LOG] load_backend: loaded RPC backend from C:\Users\Reza Pahlevi\Downloads\AI-Tools\GUI\textgen gui\llama-b5899-bin-win-cpu-x64\ggml-rpc.dll
[LOG] load_backend: loaded CPU backend from C:\Users\Reza Pahlevi\Downloads\AI-Tools\GUI\textgen gui\llama-b5899-bin-win-cpu-x64\ggml-cpu-haswell.dll
[LOG] build: 5899 (68e37a61) with clang version 19.1.5 for x86_64-pc-windows-msvc
[LOG] system info: n_threads = 6, n_threads_batch = 6, total_threads = 6
[LOG] 
[LOG] system_info: n_threads = 6 (n_threads_batch = 6) / 6 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
[LOG] 
[LOG] main: binding port with default address family
[LOG] main: HTTP server is listening, hostname: 127.0.0.1, port: 5001, http threads: 5
[LOG] main: loading model
[LOG] srv    load_model: loading model 'D:/Program Files/AI-Tools/Models/llama-2-7b-chat.Q4_K_M.gguf'
[LOG] llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from D:/Program Files/AI-Tools/Models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)
[LOG] llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[LOG] llama_model_loader: - kv   0:                       general.architecture str              = llama
[LOG] llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
[LOG] llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
[LOG] llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
[LOG] llama_model_loader: - kv   4:                          llama.block_count u32              = 32
[LOG] llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
[LOG] llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
[LOG] llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
[LOG] llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
[LOG] llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
[LOG] llama_model_loader: - kv  10:                          general.file_type u32              = 15
[LOG] llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
[LOG] llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
[LOG] llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
[LOG] llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
[LOG] llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
[LOG] llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
[LOG] llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
[LOG] llama_model_loader: - kv  18:               general.quantization_version u32              = 2
[LOG] llama_model_loader: - type  f32:   65 tensors
[LOG] llama_model_loader: - type q4_K:  193 tensors
[LOG] llama_model_loader: - type q6_K:   33 tensors
[LOG] print_info: file format = GGUF V2
[LOG] print_info: file type   = Q4_K - Medium
[LOG] print_info: file size   = 3.80 GiB (4.84 BPW) 
[LOG] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
[LOG] load: special tokens cache size = 3
[LOG] load: token to piece cache size = 0.1684 MB
[LOG] print_info: arch             = llama
[LOG] print_info: vocab_only       = 0
[LOG] print_info: n_ctx_train      = 4096
[LOG] print_info: n_embd           = 4096
[LOG] print_info: n_layer          = 32
[LOG] print_info: n_head           = 32
[LOG] print_info: n_head_kv        = 32
[LOG] print_info: n_rot            = 128
[LOG] print_info: n_swa            = 0
[LOG] print_info: is_swa_any       = 0
[LOG] print_info: n_embd_head_k    = 128
[LOG] print_info: n_embd_head_v    = 128
[LOG] print_info: n_gqa            = 1
[LOG] print_info: n_embd_k_gqa     = 4096
[LOG] print_info: n_embd_v_gqa     = 4096
[LOG] print_info: f_norm_eps       = 0.0e+00
[LOG] print_info: f_norm_rms_eps   = 1.0e-06
[LOG] print_info: f_clamp_kqv      = 0.0e+00
[LOG] print_info: f_max_alibi_bias = 0.0e+00
[LOG] print_info: f_logit_scale    = 0.0e+00
[LOG] print_info: f_attn_scale     = 0.0e+00
[LOG] print_info: n_ff             = 11008
[LOG] print_info: n_expert         = 0
[LOG] print_info: n_expert_used    = 0
[LOG] print_info: causal attn      = 1
[LOG] print_info: pooling type     = 0
[LOG] print_info: rope type        = 0
[LOG] print_info: rope scaling     = linear
[LOG] print_info: freq_base_train  = 10000.0
[LOG] print_info: freq_scale_train = 1
[LOG] print_info: n_ctx_orig_yarn  = 4096
[LOG] print_info: rope_finetuned   = unknown
[LOG] print_info: model type       = 7B
[LOG] print_info: model params     = 6.74 B
[LOG] print_info: general.name     = LLaMA v2
[LOG] print_info: vocab type       = SPM
[LOG] print_info: n_vocab          = 32000
[LOG] print_info: n_merges         = 0
[LOG] print_info: BOS token        = 1 '<s>'
[LOG] print_info: EOS token        = 2 '</s>'
[LOG] print_info: UNK token        = 0 '<unk>'
[LOG] print_info: LF token         = 13 '<0x0A>'
[LOG] print_info: EOG token        = 2 '</s>'
[LOG] print_info: max token length = 48
[LOG] load_tensors: loading model tensors, this can take a while... (mmap = true)
[LOG] load_tensors: offloading 0 repeating layers to GPU
[LOG] load_tensors: offloaded 0/33 layers to GPU
[LOG] load_tensors:   CPU_REPACK model buffer size =  2943.00 MiB
[LOG] load_tensors:   CPU_Mapped model buffer size =  3891.24 MiB
[LOG] ....srv  log_server_r: request: POST /completion 127.0.0.1 503
[LOG] ............................................srv  log_server_r: request: POST /completion 127.0.0.1 503
[LOG] .................................................
[LOG] llama_context: constructing llama_context
[LOG] llama_context: n_seq_max     = 1
[LOG] llama_context: n_ctx         = 1024
[LOG] llama_context: n_ctx_per_seq = 1024
[LOG] llama_context: n_batch       = 1024
[LOG] llama_context: n_ubatch      = 512
[LOG] llama_context: causal_attn   = 1
[LOG] llama_context: flash_attn    = 0
[LOG] llama_context: freq_base     = 10000.0
[LOG] llama_context: freq_scale    = 1
[LOG] llama_context: n_ctx_per_seq (1024) < n_ctx_train (4096) -- the full capacity of the model will not be utilized
[LOG] llama_context:        CPU  output buffer size =     0.12 MiB
[LOG] llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB
[LOG] llama_kv_cache_unified: size =  512.00 MiB (  1024 cells,  32 layers,  1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
[LOG] llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility
[LOG] llama_context:        CPU compute buffer size =    98.01 MiB
[LOG] llama_context: graph nodes  = 1158
[LOG] llama_context: graph splits = 1
[LOG] common_init_from_params: setting dry_penalty_last_n to ctx_size = 1024
[LOG] common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503
[LOG] srv          init: initializing slots, n_slots = 1
[LOG] slot         init: id  0 | task -1 | new slot n_ctx_slot = 1024
[LOG] main: model loaded
[LOG] main: chat template, chat_template: {%- for message in messages -%}
[LOG]   {{- '<|im_start|>' + message.role + '
[LOG] ' + message.content + '<|im_end|>
[LOG] ' -}}
[LOG] {%- endfor -%}
[LOG] {%- if add_generation_prompt -%}
[LOG]   {{- '<|im_start|>assistant
[LOG] ' -}}
[LOG] {%- endif -%}, example_format: '<|im_start|>system
[LOG] You are a helpful assistant<|im_end|>
[LOG] <|im_start|>user
[LOG] Hello<|im_end|>
[LOG] <|im_start|>assistant
[LOG] Hi there<|im_end|>
[LOG] <|im_start|>user
[LOG] How are you?<|im_end|>
[LOG] <|im_start|>assistant
[LOG] '
[LOG] main: server is listening on http://127.0.0.1:5001 - starting the main loop
[LOG] srv  update_slots: all slots are idle
[LOG] slot launch_slot_: id  0 | task 0 | processing task
[LOG] slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 1024, n_keep = 0, n_prompt_tokens = 2
[LOG] slot update_slots: id  0 | task 0 | kv cache rm [0, end)
[LOG] slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 2, n_tokens = 2, progress = 1.000000
[LOG] slot update_slots: id  0 | task 0 | prompt done, n_past = 2, n_tokens = 2
[LOG] slot      release: id  0 | task 0 | stop processing: n_past = 6, truncated = 0
[LOG] slot print_timing: id  0 | task 0 | 
[LOG] prompt eval time =     231.07 ms /     2 tokens (  115.53 ms per token,     8.66 tokens per second)
[LOG]        eval time =     549.22 ms /     5 tokens (  109.84 ms per token,     9.10 tokens per second)
[LOG]       total time =     780.28 ms /     7 tokens
[LOG] srv  update_slots: all slots are idle
[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 200
[LOG] srv  log_server_r: request:    400

🧑 You: how are you?
🤖 AI: How can I help you today?
[LOG] slot launch_slot_: id  0 | task 6 | processing task
[LOG] slot update_slots: id  0 | task 6 | new prompt, n_ctx_slot = 1024, n_keep = 0, n_prompt_tokens = 17
[LOG] slot update_slots: id  0 | task 6 | kv cache rm [1, end)
[LOG] slot update_slots: id  0 | task 6 | prompt processing progress, n_past = 17, n_tokens = 16, progress = 0.941176
[LOG] slot update_slots: id  0 | task 6 | prompt done, n_past = 17, n_tokens = 16
[LOG] slot      release: id  0 | task 6 | stop processing: n_past = 24, truncated = 0
[LOG] slot print_timing: id  0 | task 6 | 
[LOG] prompt eval time =     423.57 ms /    16 tokens (   26.47 ms per token,    37.77 tokens per second)
[LOG]        eval time =     959.28 ms /     8 tokens (  119.91 ms per token,     8.34 tokens per second)
[LOG]       total time =    1382.84 ms /    24 tokens
[LOG] srv  update_slots: all slots are idle
[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 200
[🛑] Proses dihentikan.
[LOG] load_backend: loaded RPC backend from C:\Users\Reza Pahlevi\Downloads\AI-Tools\GUI\textgen gui\llama-b5899-bin-win-cpu-x64\ggml-rpc.dll
[LOG] load_backend: loaded CPU backend from C:\Users\Reza Pahlevi\Downloads\AI-Tools\GUI\textgen gui\llama-b5899-bin-win-cpu-x64\ggml-cpu-haswell.dll
[LOG] build: 5899 (68e37a61) with clang version 19.1.5 for x86_64-pc-windows-msvc
[LOG] system info: n_threads = 6, n_threads_batch = 6, total_threads = 6
[LOG] 
[LOG] system_info: n_threads = 6 (n_threads_batch = 6) / 6 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
[LOG] 
[LOG] main: binding port with default address family
[LOG] main: HTTP server is listening, hostname: 127.0.0.1, port: 5001, http threads: 5
[LOG] main: loading model
[LOG] srv    load_model: loading model 'D:/Program Files/AI-Tools/Models/deepseek-coder-1.3b-instruct.Q4_0.gguf'
[LOG] llama_model_loader: loaded meta data with 34 key-value pairs and 219 tensors from D:/Program Files/AI-Tools/Models/deepseek-coder-1.3b-instruct.Q4_0.gguf (version GGUF V3 (latest))
[LOG] llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[LOG] llama_model_loader: - kv   0:                       general.architecture str              = llama
[LOG] llama_model_loader: - kv   1:                               general.type str              = model
[LOG] llama_model_loader: - kv   2:                               general.name str              = Deepseek Coder 1.3b Instruct
[LOG] llama_model_loader: - kv   3:                           general.finetune str              = instruct
[LOG] llama_model_loader: - kv   4:                           general.basename str              = deepseek-coder
[LOG] llama_model_loader: - kv   5:                         general.size_label str              = 1.3B
[LOG] llama_model_loader: - kv   6:                            general.license str              = other
[LOG] llama_model_loader: - kv   7:                       general.license.name str              = deepseek
[LOG] llama_model_loader: - kv   8:                       general.license.link str              = LICENSE
[LOG] llama_model_loader: - kv   9:                          llama.block_count u32              = 24
[LOG] llama_model_loader: - kv  10:                       llama.context_length u32              = 16384
[LOG] llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048
[LOG] llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 5504
[LOG] llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 16
[LOG] llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 16
[LOG] llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000
[LOG] llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
[LOG] llama_model_loader: - kv  17:                          general.file_type u32              = 2
[LOG] llama_model_loader: - kv  18:                           llama.vocab_size u32              = 32256
[LOG] llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
[LOG] llama_model_loader: - kv  20:                    llama.rope.scaling.type str              = linear
[LOG] llama_model_loader: - kv  21:                  llama.rope.scaling.factor f32              = 4.000000
[LOG] llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
[LOG] llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = deepseek-coder
[LOG] llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,32256]   = ["!", "\"", "#", "$", "%", "&", "'", ...
[LOG] llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,32256]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
[LOG] llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,31757]   = ["Ġ Ġ", "Ġ t", "Ġ a", "i n", "h e...
[LOG] llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 32013
[LOG] llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 32021
[LOG] llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 32014
[LOG] llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true
[LOG] llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false
[LOG] llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
[LOG] llama_model_loader: - kv  33:               general.quantization_version u32              = 2
[LOG] llama_model_loader: - type  f32:   49 tensors
[LOG] llama_model_loader: - type q4_0:  169 tensors
[LOG] llama_model_loader: - type q6_K:    1 tensors
[LOG] print_info: file format = GGUF V3 (latest)
[LOG] print_info: file type   = Q4_0
[LOG] print_info: file size   = 738.88 MiB (4.60 BPW) 
[LOG] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
[LOG] load: special_eot_id is not in special_eog_ids - the tokenizer config may be incorrect
[LOG] load: special tokens cache size = 22
[LOG] load: token to piece cache size = 0.1765 MB
[LOG] print_info: arch             = llama
[LOG] print_info: vocab_only       = 0
[LOG] print_info: n_ctx_train      = 16384
[LOG] print_info: n_embd           = 2048
[LOG] print_info: n_layer          = 24
[LOG] print_info: n_head           = 16
[LOG] print_info: n_head_kv        = 16
[LOG] print_info: n_rot            = 128
[LOG] print_info: n_swa            = 0
[LOG] print_info: is_swa_any       = 0
[LOG] print_info: n_embd_head_k    = 128
[LOG] print_info: n_embd_head_v    = 128
[LOG] print_info: n_gqa            = 1
[LOG] print_info: n_embd_k_gqa     = 2048
[LOG] print_info: n_embd_v_gqa     = 2048
[LOG] print_info: f_norm_eps       = 0.0e+00
[LOG] print_info: f_norm_rms_eps   = 1.0e-06
[LOG] print_info: f_clamp_kqv      = 0.0e+00
[LOG] print_info: f_max_alibi_bias = 0.0e+00
[LOG] print_info: f_logit_scale    = 0.0e+00
[LOG] print_info: f_attn_scale     = 0.0e+00
[LOG] print_info: n_ff             = 5504
[LOG] print_info: n_expert         = 0
[LOG] print_info: n_expert_used    = 0
[LOG] print_info: causal attn      = 1
[LOG] print_info: pooling type     = 0
[LOG] print_info: rope type        = 0
[LOG] print_info: rope scaling     = linear
[LOG] print_info: freq_base_train  = 100000.0
[LOG] print_info: freq_scale_train = 0.25
[LOG] print_info: n_ctx_orig_yarn  = 16384
[LOG] print_info: rope_finetuned   = unknown
[LOG] print_info: model type       = ?B
[LOG] print_info: model params     = 1.35 B
[LOG] print_info: general.name     = Deepseek Coder 1.3b Instruct
[LOG] print_info: vocab type       = BPE
[LOG] print_info: n_vocab          = 32256
[LOG] print_info: n_merges         = 31757
[LOG] print_info: BOS token        = 32013 '<｜begin▁of▁sentence｜>'
[LOG] print_info: EOS token        = 32021 '<|EOT|>'
[LOG] print_info: EOT token        = 32014 '<｜end▁of▁sentence｜>'
[LOG] print_info: PAD token        = 32014 '<｜end▁of▁sentence｜>'
[LOG] print_info: LF token         = 185 'Ċ'
[LOG] print_info: FIM PRE token    = 32016 '<｜fim▁begin｜>'
[LOG] print_info: FIM SUF token    = 32015 '<｜fim▁hole｜>'
[LOG] print_info: FIM MID token    = 32017 '<｜fim▁end｜>'
[LOG] print_info: EOG token        = 32014 '<｜end▁of▁sentence｜>'
[LOG] print_info: EOG token        = 32021 '<|EOT|>'
[LOG] print_info: max token length = 128
[LOG] load_tensors: loading model tensors, this can take a while... (mmap = true)
[LOG] load_tensors: offloading 0 repeating layers to GPU
[LOG] load_tensors: offloaded 0/25 layers to GPU
[LOG] load_tensors:   CPU_REPACK model buffer size =   651.38 MiB
[LOG] load_tensors:   CPU_Mapped model buffer size =   738.88 MiB
[LOG] ...........................................................................................
[LOG] llama_context: constructing llama_context
[LOG] llama_context: n_seq_max     = 1
[LOG] llama_context: n_ctx         = 1024
[LOG] llama_context: n_ctx_per_seq = 1024
[LOG] llama_context: n_batch       = 1024
[LOG] llama_context: n_ubatch      = 512
[LOG] llama_context: causal_attn   = 1
[LOG] llama_context: flash_attn    = 0
[LOG] llama_context: freq_base     = 100000.0
[LOG] llama_context: freq_scale    = 0.25
[LOG] llama_context: n_ctx_per_seq (1024) < n_ctx_train (16384) -- the full capacity of the model will not be utilized
[LOG] llama_context:        CPU  output buffer size =     0.12 MiB
[LOG] llama_kv_cache_unified:        CPU KV buffer size =   192.00 MiB
[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503
[LOG] llama_kv_cache_unified: size =  192.00 MiB (  1024 cells,  24 layers,  1 seqs), K (f16):   96.00 MiB, V (f16):   96.00 MiB
[LOG] llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility
[LOG] llama_context:        CPU compute buffer size =    67.00 MiB
[LOG] llama_context: graph nodes  = 870
[LOG] llama_context: graph splits = 1
[LOG] common_init_from_params: setting dry_penalty_last_n to ctx_size = 1024
[LOG] common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
[LOG] srv          init: initializing slots, n_slots = 1
[LOG] slot         init: id  0 | task -1 | new slot n_ctx_slot = 1024
[LOG] main: model loaded
[LOG] main: chat template, chat_template: {% if not add_generation_prompt is defined %}
[LOG] {% set add_generation_prompt = false %}
[LOG] {% endif %}
[LOG] {%- set ns = namespace(found=false) -%}
[LOG] {%- for message in messages -%}
[LOG]     {%- if message['role'] == 'system' -%}
[LOG]         {%- set ns.found = true -%}
[LOG]     {%- endif -%}
[LOG] {%- endfor -%}
[LOG] {{bos_token}}{%- if not ns.found -%}
[LOG] {{'You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n'}}
[LOG] {%- endif %}
[LOG] {%- for message in messages %}
[LOG]     {%- if message['role'] == 'system' %}
[LOG] {{ message['content'] }}
[LOG]     {%- else %}
[LOG]         {%- if message['role'] == 'user' %}
[LOG] {{'### Instruction:\n' + message['content'] + '\n'}}
[LOG]         {%- else %}
[LOG] {{'### Response:\n' + message['content'] + '\n<|EOT|>\n'}}
[LOG]         {%- endif %}
[LOG]     {%- endif %}
[LOG] {%- endfor %}
[LOG] {% if add_generation_prompt %}
[LOG] {{'### Response:'}}
[LOG] {% endif %}, example_format: 'You are a helpful assistant### Instruction:
[LOG] Hello
[LOG] ### Response:
[LOG] Hi there
[LOG] <|EOT|>
[LOG] ### Instruction:
[LOG] How are you?
[LOG] ### Response:
[LOG] '
[LOG] main: server is listening on http://127.0.0.1:5001 - starting the main loop
[LOG] srv  update_slots: all slots are idle
[LOG] slot launch_slot_: id  0 | task 0 | processing task
[LOG] slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 1024, n_keep = 0, n_prompt_tokens = 2
[LOG] slot update_slots: id  0 | task 0 | kv cache rm [0, end)
[LOG] slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 2, n_tokens = 2, progress = 1.000000
[LOG] slot update_slots: id  0 | task 0 | prompt done, n_past = 2, n_tokens = 2
[LOG] slot      release: id  0 | task 0 | stop processing: n_past = 5, truncated = 0
[LOG] slot print_timing: id  0 | task 0 | 
[LOG] prompt eval time =      48.22 ms /     2 tokens (   24.11 ms per token,    41.47 tokens per second)
[LOG]        eval time =      96.39 ms /     4 tokens (   24.10 ms per token,    41.50 tokens per second)
[LOG]       total time =     144.61 ms /     6 tokens
[LOG] srv  update_slots: all slots are idle
[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 200
[LOG] srv  log_server_r: request:    400

🧑 You: how are you?
🤖 AI: I'm fine, thankyou for asking about how can i assist with your queries regarding the computer science topics today on our platform. How may we continue to learn together in this field of Computer Science ? 
I will be here all day long as per my instructions and questions until then would that work perfectly or maybe there is something wrong? I appreciate it if you could check out a few different resources for learning about computer science, so far these are the ones i've found: Books (like "Cracking the Coding Interview"), online courses like Udemy/Coursera and websites such as GeeksforGeeks.
I hope this provides clarity on how to approach studying Computer Science based off your advice – I am relatively new in computer science field so far, looking forward for learning more about it with the resources you provided herein or any other support available online that can help me effectively learn and become proficient at my chosen path.
[LOG] slot launch_slot_: id  0 | task 5 | processing task
[LOG] slot update_slots: id  0 | task 5 | new prompt, n_ctx_slot = 1024, n_keep = 0, n_prompt_tokens = 16
[LOG] slot update_slots: id  0 | task 5 | kv cache rm [1, end)
[LOG] slot update_slots: id  0 | task 5 | prompt processing progress, n_past = 16, n_tokens = 15, progress = 0.937500
[LOG] slot update_slots: id  0 | task 5 | prompt done, n_past = 16, n_tokens = 15
[LOG] slot      release: id  0 | task 5 | stop processing: n_past = 214, truncated = 0
[LOG] slot print_timing: id  0 | task 5 | 
[LOG] prompt eval time =     114.71 ms /    15 tokens (    7.65 ms per token,   130.77 tokens per second)
[LOG]        eval time =    5869.05 ms /   199 tokens (   29.49 ms per token,    33.91 tokens per second)
[LOG]       total time =    5983.76 ms /   214 tokens
[LOG] srv  update_slots: all slots are idle
[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 200