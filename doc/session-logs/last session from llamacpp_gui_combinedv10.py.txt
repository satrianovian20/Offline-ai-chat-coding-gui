ðŸ’¬ Selamat datang!
ðŸ’¬ Selamat datang!
ðŸ’¬ Selamat datang!
[LOG] load_backend: loaded RPC backend from C:\Users\Reza Pahlevi\Downloads\AI-Tools\GUI\textgen gui\llama-b5899-bin-win-cpu-x64\ggml-rpc.dll

[LOG] load_backend: loaded CPU backend from C:\Users\Reza Pahlevi\Downloads\AI-Tools\GUI\textgen gui\llama-b5899-bin-win-cpu-x64\ggml-cpu-haswell.dll

[LOG] build: 5899 (68e37a61) with clang version 19.1.5 for x86_64-pc-windows-msvc

[LOG] system info: n_threads = 6, n_threads_batch = 6, total_threads = 6

[LOG] 

[LOG] system_info: n_threads = 6 (n_threads_batch = 6) / 6 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

[LOG] 

[LOG] main: binding port with default address family

[LOG] main: HTTP server is listening, hostname: 127.0.0.1, port: 5001, http threads: 5

[LOG] main: loading model

[LOG] srv    load_model: loading model 'D:/Program Files/AI-Tools/Models/deepseek-coder-6.7b-instruct.Q4_K_M.gguf'

[LOG] llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from D:/Program Files/AI-Tools/Models/deepseek-coder-6.7b-instruct.Q4_K_M.gguf (version GGUF V3 (latest))

[LOG] llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.

[LOG] llama_model_loader: - kv   0:                       general.architecture str              = llama

[LOG] llama_model_loader: - kv   1:                               general.name str              = deepseek-ai_deepseek-coder-6.7b-instruct

[LOG] llama_model_loader: - kv   2:                       llama.context_length u32              = 16384

[LOG] llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096

[LOG] llama_model_loader: - kv   4:                          llama.block_count u32              = 32

[LOG] llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008

[LOG] llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128

[LOG] llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32

[LOG] llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32

[LOG] llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001

[LOG] llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 100000.000000

[LOG] llama_model_loader: - kv  11:                    llama.rope.scale_linear f32              = 4.000000

[LOG] llama_model_loader: - kv  12:                          general.file_type u32              = 15

[LOG] llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2

[LOG] llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32256]   = ["!", "\"", "#", "$", "%", "&", "'", ...

[LOG] llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32256]   = [0.000000, 0.000000, 0.000000, 0.0000...

[LOG] llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32256]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...

[LOG] llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,31757]   = ["Ä  Ä ", "Ä  t", "Ä  a", "i n", "h e...

[LOG] llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 32013

[LOG] llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 32021

[LOG] llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 32014

[LOG] llama_model_loader: - kv  21:               general.quantization_version u32              = 2

[LOG] llama_model_loader: - type  f32:   65 tensors

[LOG] llama_model_loader: - type q4_K:  193 tensors

[LOG] llama_model_loader: - type q6_K:   33 tensors

[LOG] print_info: file format = GGUF V3 (latest)

[LOG] print_info: file type   = Q4_K - Medium

[LOG] print_info: file size   = 3.80 GiB (4.84 BPW) 

[LOG] load: missing pre-tokenizer type, using: 'default'

[LOG] load:                                             

[LOG] load: ************************************        

[LOG] load: GENERATION QUALITY WILL BE DEGRADED!        

[LOG] load: CONSIDER REGENERATING THE MODEL             

[LOG] load: ************************************        

[LOG] load:                                             

[LOG] load: control-looking token:  32015 '<ï½œfimâ–holeï½œ>' was not control-type; this is probably a bug in the model. its type will be overridden

[LOG] load: control-looking token:  32017 '<ï½œfimâ–endï½œ>' was not control-type; this is probably a bug in the model. its type will be overridden

[LOG] load: control-looking token:  32016 '<ï½œfimâ–beginï½œ>' was not control-type; this is probably a bug in the model. its type will be overridden

[LOG] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

[LOG] load: special_eot_id is not in special_eog_ids - the tokenizer config may be incorrect

[LOG] load: special tokens cache size = 240

[LOG] load: token to piece cache size = 0.1792 MB

[LOG] print_info: arch             = llama

[LOG] print_info: vocab_only       = 0

[LOG] print_info: n_ctx_train      = 16384

[LOG] print_info: n_embd           = 4096

[LOG] print_info: n_layer          = 32

[LOG] print_info: n_head           = 32

[LOG] print_info: n_head_kv        = 32

[LOG] print_info: n_rot            = 128

[LOG] print_info: n_swa            = 0

[LOG] print_info: is_swa_any       = 0

[LOG] print_info: n_embd_head_k    = 128

[LOG] print_info: n_embd_head_v    = 128

[LOG] print_info: n_gqa            = 1

[LOG] print_info: n_embd_k_gqa     = 4096

[LOG] print_info: n_embd_v_gqa     = 4096

[LOG] print_info: f_norm_eps       = 0.0e+00

[LOG] print_info: f_norm_rms_eps   = 1.0e-06

[LOG] print_info: f_clamp_kqv      = 0.0e+00

[LOG] print_info: f_max_alibi_bias = 0.0e+00

[LOG] print_info: f_logit_scale    = 0.0e+00

[LOG] print_info: f_attn_scale     = 0.0e+00

[LOG] print_info: n_ff             = 11008

[LOG] print_info: n_expert         = 0

[LOG] print_info: n_expert_used    = 0

[LOG] print_info: causal attn      = 1

[LOG] print_info: pooling type     = 0

[LOG] print_info: rope type        = 0

[LOG] print_info: rope scaling     = linear

[LOG] print_info: freq_base_train  = 100000.0

[LOG] print_info: freq_scale_train = 0.25

[LOG] print_info: n_ctx_orig_yarn  = 16384

[LOG] print_info: rope_finetuned   = unknown

[LOG] print_info: model type       = 7B

[LOG] print_info: model params     = 6.74 B

[LOG] print_info: general.name     = deepseek-ai_deepseek-coder-6.7b-instruct

[LOG] print_info: vocab type       = BPE

[LOG] print_info: n_vocab          = 32256

[LOG] print_info: n_merges         = 31757

[LOG] print_info: BOS token        = 32013 '<ï½œbeginâ–ofâ–sentenceï½œ>'

[LOG] print_info: EOS token        = 32021 '<|EOT|>'

[LOG] print_info: EOT token        = 32014 '<ï½œendâ–ofâ–sentenceï½œ>'

[LOG] print_info: PAD token        = 32014 '<ï½œendâ–ofâ–sentenceï½œ>'

[LOG] print_info: LF token         = 185 'ÄŠ'

[LOG] print_info: FIM PRE token    = 32016 '<ï½œfimâ–beginï½œ>'

[LOG] print_info: FIM SUF token    = 32015 '<ï½œfimâ–holeï½œ>'

[LOG] print_info: FIM MID token    = 32017 '<ï½œfimâ–endï½œ>'

[LOG] print_info: EOG token        = 32014 '<ï½œendâ–ofâ–sentenceï½œ>'

[LOG] print_info: EOG token        = 32021 '<|EOT|>'

[LOG] print_info: max token length = 128

[LOG] load_tensors: loading model tensors, this can take a while... (mmap = true)

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] srv  log_server_r: request:    400

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] load_tensors: offloading 0 repeating layers to GPU

[LOG] load_tensors: offloaded 0/33 layers to GPU

[LOG] load_tensors:   CPU_REPACK model buffer size =  2943.00 MiB

[LOG] load_tensors:   CPU_Mapped model buffer size =  3892.62 MiB

[LOG] ....srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] srv  log_server_r: request:    400

[LOG] .................................srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] ..................................srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] ..........................

[LOG] llama_context: constructing llama_context

[LOG] llama_context: n_seq_max     = 1

[LOG] llama_context: n_ctx         = 4096

[LOG] llama_context: n_ctx_per_seq = 4096

[LOG] llama_context: n_batch       = 2048

[LOG] llama_context: n_ubatch      = 512

[LOG] llama_context: causal_attn   = 1

[LOG] llama_context: flash_attn    = 0

[LOG] llama_context: freq_base     = 100000.0

[LOG] llama_context: freq_scale    = 0.25

[LOG] llama_context: n_ctx_per_seq (4096) < n_ctx_train (16384) -- the full capacity of the model will not be utilized

[LOG] llama_context:        CPU  output buffer size =     0.12 MiB

[LOG] llama_kv_cache_unified:        CPU KV buffer size =  2048.00 MiB

[LOG] llama_kv_cache_unified: size = 2048.00 MiB (  4096 cells,  32 layers,  1 seqs), K (f16): 1024.00 MiB, V (f16): 1024.00 MiB

[LOG] llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility

[LOG] llama_context:        CPU compute buffer size =   296.01 MiB

[LOG] llama_context: graph nodes  = 1158

[LOG] llama_context: graph splits = 1

[LOG] common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096

[LOG] common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] srv  log_server_r: request:    400

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] srv          init: initializing slots, n_slots = 1

[LOG] slot         init: id  0 | task -1 | new slot n_ctx_slot = 4096

[LOG] main: model loaded

[LOG] main: chat template, chat_template: {%- for message in messages -%}

[LOG]   {{- '<|im_start|>' + message.role + '

[LOG] ' + message.content + '<|im_end|>

[LOG] ' -}}

[LOG] {%- endfor -%}

[LOG] {%- if add_generation_prompt -%}

[LOG]   {{- '<|im_start|>assistant

[LOG] ' -}}

[LOG] {%- endif -%}, example_format: '<|im_start|>system

[LOG] You are a helpful assistant<|im_end|>

[LOG] <|im_start|>user

[LOG] Hello<|im_end|>

[LOG] <|im_start|>assistant

[LOG] Hi there<|im_end|>

[LOG] <|im_start|>user

[LOG] How are you?<|im_end|>

[LOG] <|im_start|>assistant

[LOG] '

[LOG] main: server is listening on http://127.0.0.1:5001 - starting the main loop

[LOG] srv  update_slots: all slots are idle

[LOG] slot launch_slot_: id  0 | task 0 | processing task

[LOG] slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 1

[LOG] slot update_slots: id  0 | task 0 | kv cache rm [0, end)

[LOG] slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 1, n_tokens = 1, progress = 1.000000

[LOG] slot update_slots: id  0 | task 0 | prompt done, n_past = 1, n_tokens = 1

[LOG] slot      release: id  0 | task 0 | stop processing: n_past = 5, truncated = 0

[LOG] slot print_timing: id  0 | task 0 | 

[LOG] prompt eval time =     307.73 ms /     1 tokens (  307.73 ms per token,     3.25 tokens per second)

[LOG]        eval time =     898.60 ms /     5 tokens (  179.72 ms per token,     5.56 tokens per second)

[LOG]       total time =    1206.33 ms /     6 tokens

[LOG] srv  update_slots: all slots are idle

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 200


ðŸ§‘ You: hello, how are you?
ðŸ¤– AI: I'm fine, thank you. How can I assist you today?
```

## Features:

1) Open Source: You have the freedom to modify or distribute this software as per your needs under GNU General Public License v3.0 which is compatible with Apache license 2.0.

2) Easy-to-use: An easy and simple interface allows you... (ringkas)
[LOG] slot launch_slot_: id  0 | task 6 | processing task

[LOG] slot update_slots: id  0 | task 6 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 17

[LOG] slot update_slots: id  0 | task 6 | kv cache rm [0, end)

[LOG] slot update_slots: id  0 | task 6 | prompt processing progress, n_past = 17, n_tokens = 17, progress = 1.000000

[LOG] slot update_slots: id  0 | task 6 | prompt done, n_past = 17, n_tokens = 17

[LOG] slot      release: id  0 | task 6 | stop processing: n_past = 216, truncated = 0

[LOG] slot print_timing: id  0 | task 6 | 

[LOG] prompt eval time =    1349.17 ms /    17 tokens (   79.36 ms per token,    12.60 tokens per second)

[LOG]        eval time =   49112.53 ms /   200 tokens (  245.56 ms per token,     4.07 tokens per second)

[LOG]       total time =   50461.70 ms /   217 tokens

[LOG] srv  update_slots: all slots are idle

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 200


ðŸ§‘ You: explain html
ðŸ¤– AI: 5: HTML5 is the latest evolution of the standard that defines HTML. It brings improved features for creating rich content, like drag-and-drop, video playback, etc., which makes it easier to use. It also provides APIs for media access, geolocation, and more.
```

This chatbot can be easily extended w... (ringkas)
[LOG] slot launch_slot_: id  0 | task 207 | processing task

[LOG] slot update_slots: id  0 | task 207 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 14

[LOG] slot update_slots: id  0 | task 207 | kv cache rm [11, end)

[LOG] slot update_slots: id  0 | task 207 | prompt processing progress, n_past = 14, n_tokens = 3, progress = 0.214286

[LOG] slot update_slots: id  0 | task 207 | prompt done, n_past = 14, n_tokens = 3

[LOG] slot      release: id  0 | task 207 | stop processing: n_past = 213, truncated = 0

[LOG] slot print_timing: id  0 | task 207 | 

[LOG] prompt eval time =     519.51 ms /     3 tokens (  173.17 ms per token,     5.77 tokens per second)

[LOG]        eval time =   40810.51 ms /   200 tokens (  204.05 ms per token,     4.90 tokens per second)

[LOG]       total time =   41330.02 ms /   203 tokens

[LOG] srv  update_slots: all slots are idle

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 200


ðŸ§‘ You: explain css
ðŸ¤– AI: ```
CSS stands for Cascading Style Sheets. It is used to style HTML elements on your web page. CSS describes how HTML elements should be displayed including layout, colors, fonts etc. This separation of content (HTML) from design (CSS) makes it easy to make changes and saves a lot of work.
```

```q... (ringkas)

[LOG] load_backend: loaded RPC backend from C:\Users\Reza Pahlevi\Downloads\AI-Tools\GUI\textgen gui\llama-b5899-bin-win-cpu-x64\ggml-rpc.dll

[LOG] load_backend: loaded CPU backend from C:\Users\Reza Pahlevi\Downloads\AI-Tools\GUI\textgen gui\llama-b5899-bin-win-cpu-x64\ggml-cpu-haswell.dll

[LOG] build: 5899 (68e37a61) with clang version 19.1.5 for x86_64-pc-windows-msvc

[LOG] system info: n_threads = 6, n_threads_batch = 6, total_threads = 6

[LOG] 

[LOG] system_info: n_threads = 6 (n_threads_batch = 6) / 6 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

[LOG] 

[LOG] main: binding port with default address family

[LOG] main: HTTP server is listening, hostname: 127.0.0.1, port: 5001, http threads: 5

[LOG] main: loading model

[LOG] srv    load_model: loading model 'D:/Program Files/AI-Tools/Models/deepseek-coder-6.7b-instruct.Q4_K_M.gguf'

[LOG] llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from D:/Program Files/AI-Tools/Models/deepseek-coder-6.7b-instruct.Q4_K_M.gguf (version GGUF V3 (latest))

[LOG] llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.

[LOG] llama_model_loader: - kv   0:                       general.architecture str              = llama

[LOG] llama_model_loader: - kv   1:                               general.name str              = deepseek-ai_deepseek-coder-6.7b-instruct

[LOG] llama_model_loader: - kv   2:                       llama.context_length u32              = 16384

[LOG] llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096

[LOG] llama_model_loader: - kv   4:                          llama.block_count u32              = 32

[LOG] llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008

[LOG] llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128

[LOG] llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32

[LOG] llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32

[LOG] llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001

[LOG] llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 100000.000000

[LOG] llama_model_loader: - kv  11:                    llama.rope.scale_linear f32              = 4.000000

[LOG] llama_model_loader: - kv  12:                          general.file_type u32              = 15

[LOG] llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2

[LOG] llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32256]   = ["!", "\"", "#", "$", "%", "&", "'", ...

[LOG] llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32256]   = [0.000000, 0.000000, 0.000000, 0.0000...

[LOG] llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32256]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...

[LOG] llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,31757]   = ["Ä  Ä ", "Ä  t", "Ä  a", "i n", "h e...

[LOG] llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 32013

[LOG] llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 32021

[LOG] llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 32014

[LOG] llama_model_loader: - kv  21:               general.quantization_version u32              = 2

[LOG] llama_model_loader: - type  f32:   65 tensors

[LOG] llama_model_loader: - type q4_K:  193 tensors

[LOG] llama_model_loader: - type q6_K:   33 tensors

[LOG] print_info: file format = GGUF V3 (latest)

[LOG] print_info: file type   = Q4_K - Medium

[LOG] print_info: file size   = 3.80 GiB (4.84 BPW) 

[LOG] load: missing pre-tokenizer type, using: 'default'

[LOG] load:                                             

[LOG] load: ************************************        

[LOG] load: GENERATION QUALITY WILL BE DEGRADED!        

[LOG] load: CONSIDER REGENERATING THE MODEL             

[LOG] load: ************************************        

[LOG] load:                                             

[LOG] load: control-looking token:  32015 '<ï½œfimâ–holeï½œ>' was not control-type; this is probably a bug in the model. its type will be overridden

[LOG] load: control-looking token:  32017 '<ï½œfimâ–endï½œ>' was not control-type; this is probably a bug in the model. its type will be overridden

[LOG] load: control-looking token:  32016 '<ï½œfimâ–beginï½œ>' was not control-type; this is probably a bug in the model. its type will be overridden

[LOG] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

[LOG] load: special_eot_id is not in special_eog_ids - the tokenizer config may be incorrect

[LOG] load: special tokens cache size = 240

[LOG] load: token to piece cache size = 0.1792 MB

[LOG] print_info: arch             = llama

[LOG] print_info: vocab_only       = 0

[LOG] print_info: n_ctx_train      = 16384

[LOG] print_info: n_embd           = 4096

[LOG] print_info: n_layer          = 32

[LOG] print_info: n_head           = 32

[LOG] print_info: n_head_kv        = 32

[LOG] print_info: n_rot            = 128

[LOG] print_info: n_swa            = 0

[LOG] print_info: is_swa_any       = 0

[LOG] print_info: n_embd_head_k    = 128

[LOG] print_info: n_embd_head_v    = 128

[LOG] print_info: n_gqa            = 1

[LOG] print_info: n_embd_k_gqa     = 4096

[LOG] print_info: n_embd_v_gqa     = 4096

[LOG] print_info: f_norm_eps       = 0.0e+00

[LOG] print_info: f_norm_rms_eps   = 1.0e-06

[LOG] print_info: f_clamp_kqv      = 0.0e+00

[LOG] print_info: f_max_alibi_bias = 0.0e+00

[LOG] print_info: f_logit_scale    = 0.0e+00

[LOG] print_info: f_attn_scale     = 0.0e+00

[LOG] print_info: n_ff             = 11008

[LOG] print_info: n_expert         = 0

[LOG] print_info: n_expert_used    = 0

[LOG] print_info: causal attn      = 1

[LOG] print_info: pooling type     = 0

[LOG] print_info: rope type        = 0

[LOG] print_info: rope scaling     = linear

[LOG] print_info: freq_base_train  = 100000.0

[LOG] print_info: freq_scale_train = 0.25

[LOG] print_info: n_ctx_orig_yarn  = 16384

[LOG] print_info: rope_finetuned   = unknown

[LOG] print_info: model type       = 7B

[LOG] print_info: model params     = 6.74 B

[LOG] print_info: general.name     = deepseek-ai_deepseek-coder-6.7b-instruct

[LOG] print_info: vocab type       = BPE

[LOG] print_info: n_vocab          = 32256

[LOG] print_info: n_merges         = 31757

[LOG] print_info: BOS token        = 32013 '<ï½œbeginâ–ofâ–sentenceï½œ>'

[LOG] print_info: EOS token        = 32021 '<|EOT|>'

[LOG] print_info: EOT token        = 32014 '<ï½œendâ–ofâ–sentenceï½œ>'

[LOG] print_info: PAD token        = 32014 '<ï½œendâ–ofâ–sentenceï½œ>'

[LOG] print_info: LF token         = 185 'ÄŠ'

[LOG] print_info: FIM PRE token    = 32016 '<ï½œfimâ–beginï½œ>'

[LOG] print_info: FIM SUF token    = 32015 '<ï½œfimâ–holeï½œ>'

[LOG] print_info: FIM MID token    = 32017 '<ï½œfimâ–endï½œ>'

[LOG] print_info: EOG token        = 32014 '<ï½œendâ–ofâ–sentenceï½œ>'

[LOG] print_info: EOG token        = 32021 '<|EOT|>'

[LOG] print_info: max token length = 128

[LOG] load_tensors: loading model tensors, this can take a while... (mmap = true)

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] load_tensors: offloading 0 repeating layers to GPU

[LOG] load_tensors: offloaded 0/33 layers to GPU

[LOG] load_tensors:   CPU_REPACK model buffer size =  2943.00 MiB

[LOG] load_tensors:   CPU_Mapped model buffer size =  3892.62 MiB

[LOG] .................srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] ....................................srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] ............................................

[LOG] llama_context: constructing llama_context

[LOG] llama_context: n_seq_max     = 1

[LOG] llama_context: n_ctx         = 4096

[LOG] llama_context: n_ctx_per_seq = 4096

[LOG] llama_context: n_batch       = 2048

[LOG] llama_context: n_ubatch      = 512

[LOG] llama_context: causal_attn   = 1

[LOG] llama_context: flash_attn    = 0

[LOG] llama_context: freq_base     = 100000.0

[LOG] llama_context: freq_scale    = 0.25

[LOG] llama_context: n_ctx_per_seq (4096) < n_ctx_train (16384) -- the full capacity of the model will not be utilized

[LOG] llama_context:        CPU  output buffer size =     0.12 MiB

[LOG] llama_kv_cache_unified:        CPU KV buffer size =  2048.00 MiB

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] srv  log_server_r: request:    400

[LOG] llama_kv_cache_unified: size = 2048.00 MiB (  4096 cells,  32 layers,  1 seqs), K (f16): 1024.00 MiB, V (f16): 1024.00 MiB

[LOG] llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility

[LOG] llama_context:        CPU compute buffer size =   296.01 MiB

[LOG] llama_context: graph nodes  = 1158

[LOG] llama_context: graph splits = 1

[LOG] common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096

[LOG] common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] srv          init: initializing slots, n_slots = 1

[LOG] slot         init: id  0 | task -1 | new slot n_ctx_slot = 4096

[LOG] main: model loaded

[LOG] main: chat template, chat_template: {%- for message in messages -%}

[LOG]   {{- '<|im_start|>' + message.role + '

[LOG] ' + message.content + '<|im_end|>

[LOG] ' -}}

[LOG] {%- endfor -%}

[LOG] {%- if add_generation_prompt -%}

[LOG]   {{- '<|im_start|>assistant

[LOG] ' -}}

[LOG] {%- endif -%}, example_format: '<|im_start|>system

[LOG] You are a helpful assistant<|im_end|>

[LOG] <|im_start|>user

[LOG] Hello<|im_end|>

[LOG] <|im_start|>assistant

[LOG] Hi there<|im_end|>

[LOG] <|im_start|>user

[LOG] How are you?<|im_end|>

[LOG] <|im_start|>assistant

[LOG] '

[LOG] main: server is listening on http://127.0.0.1:5001 - starting the main loop

[LOG] srv  update_slots: all slots are idle

[LOG] slot launch_slot_: id  0 | task 0 | processing task

[LOG] slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 1

[LOG] slot update_slots: id  0 | task 0 | kv cache rm [0, end)

[LOG] slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 1, n_tokens = 1, progress = 1.000000

[LOG] slot update_slots: id  0 | task 0 | prompt done, n_past = 1, n_tokens = 1

[LOG] slot      release: id  0 | task 0 | stop processing: n_past = 5, truncated = 0

[LOG] slot print_timing: id  0 | task 0 | 

[LOG] prompt eval time =     276.30 ms /     1 tokens (  276.30 ms per token,     3.62 tokens per second)

[LOG]        eval time =     916.49 ms /     5 tokens (  183.30 ms per token,     5.46 tokens per second)

[LOG]       total time =    1192.78 ms /     6 tokens

[LOG] srv  update_slots: all slots are idle

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 200

[ðŸŒ¾] Menjalankan dalam RAKYAT MODE: ctx-size 1024, RAM hemat!
[ðŸ›‘] Proses lama dihentikan.
[LOG] load_backend: loaded RPC backend from C:\Users\Reza Pahlevi\Downloads\AI-Tools\GUI\textgen gui\llama-b5899-bin-win-cpu-x64\ggml-rpc.dll

[LOG] load_backend: loaded CPU backend from C:\Users\Reza Pahlevi\Downloads\AI-Tools\GUI\textgen gui\llama-b5899-bin-win-cpu-x64\ggml-cpu-haswell.dll

[LOG] build: 5899 (68e37a61) with clang version 19.1.5 for x86_64-pc-windows-msvc

[LOG] system info: n_threads = 6, n_threads_batch = 6, total_threads = 6

[LOG] 

[LOG] system_info: n_threads = 6 (n_threads_batch = 6) / 6 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

[LOG] 

[LOG] main: binding port with default address family

[LOG] main: HTTP server is listening, hostname: 127.0.0.1, port: 5001, http threads: 5

[LOG] main: loading model

[LOG] srv    load_model: loading model 'D:/Program Files/AI-Tools/Models/deepseek-coder-1.3b-instruct.Q4_0.gguf'

[LOG] llama_model_loader: loaded meta data with 34 key-value pairs and 219 tensors from D:/Program Files/AI-Tools/Models/deepseek-coder-1.3b-instruct.Q4_0.gguf (version GGUF V3 (latest))

[LOG] llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.

[LOG] llama_model_loader: - kv   0:                       general.architecture str              = llama

[LOG] llama_model_loader: - kv   1:                               general.type str              = model

[LOG] llama_model_loader: - kv   2:                               general.name str              = Deepseek Coder 1.3b Instruct

[LOG] llama_model_loader: - kv   3:                           general.finetune str              = instruct

[LOG] llama_model_loader: - kv   4:                           general.basename str              = deepseek-coder

[LOG] llama_model_loader: - kv   5:                         general.size_label str              = 1.3B

[LOG] llama_model_loader: - kv   6:                            general.license str              = other

[LOG] llama_model_loader: - kv   7:                       general.license.name str              = deepseek

[LOG] llama_model_loader: - kv   8:                       general.license.link str              = LICENSE

[LOG] llama_model_loader: - kv   9:                          llama.block_count u32              = 24

[LOG] llama_model_loader: - kv  10:                       llama.context_length u32              = 16384

[LOG] llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048

[LOG] llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 5504

[LOG] llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 16

[LOG] llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 16

[LOG] llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000

[LOG] llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001

[LOG] llama_model_loader: - kv  17:                          general.file_type u32              = 2

[LOG] llama_model_loader: - kv  18:                           llama.vocab_size u32              = 32256

[LOG] llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128

[LOG] llama_model_loader: - kv  20:                    llama.rope.scaling.type str              = linear

[LOG] llama_model_loader: - kv  21:                  llama.rope.scaling.factor f32              = 4.000000

[LOG] llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2

[LOG] llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = deepseek-coder

[LOG] llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,32256]   = ["!", "\"", "#", "$", "%", "&", "'", ...

[LOG] llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,32256]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...

[LOG] llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,31757]   = ["Ä  Ä ", "Ä  t", "Ä  a", "i n", "h e...

[LOG] llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 32013

[LOG] llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 32021

[LOG] llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 32014

[LOG] llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true

[LOG] llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false

[LOG] llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...

[LOG] llama_model_loader: - kv  33:               general.quantization_version u32              = 2

[LOG] llama_model_loader: - type  f32:   49 tensors

[LOG] llama_model_loader: - type q4_0:  169 tensors

[LOG] llama_model_loader: - type q6_K:    1 tensors

[LOG] print_info: file format = GGUF V3 (latest)

[LOG] print_info: file type   = Q4_0

[LOG] print_info: file size   = 738.88 MiB (4.60 BPW) 

[LOG] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

[LOG] load: special_eot_id is not in special_eog_ids - the tokenizer config may be incorrect

[LOG] load: special tokens cache size = 22

[LOG] load: token to piece cache size = 0.1765 MB

[LOG] print_info: arch             = llama

[LOG] print_info: vocab_only       = 0

[LOG] print_info: n_ctx_train      = 16384

[LOG] print_info: n_embd           = 2048

[LOG] print_info: n_layer          = 24

[LOG] print_info: n_head           = 16

[LOG] print_info: n_head_kv        = 16

[LOG] print_info: n_rot            = 128

[LOG] print_info: n_swa            = 0

[LOG] print_info: is_swa_any       = 0

[LOG] print_info: n_embd_head_k    = 128

[LOG] print_info: n_embd_head_v    = 128

[LOG] print_info: n_gqa            = 1

[LOG] print_info: n_embd_k_gqa     = 2048

[LOG] print_info: n_embd_v_gqa     = 2048

[LOG] print_info: f_norm_eps       = 0.0e+00

[LOG] print_info: f_norm_rms_eps   = 1.0e-06

[LOG] print_info: f_clamp_kqv      = 0.0e+00

[LOG] print_info: f_max_alibi_bias = 0.0e+00

[LOG] print_info: f_logit_scale    = 0.0e+00

[LOG] print_info: f_attn_scale     = 0.0e+00

[LOG] print_info: n_ff             = 5504

[LOG] print_info: n_expert         = 0

[LOG] print_info: n_expert_used    = 0

[LOG] print_info: causal attn      = 1

[LOG] print_info: pooling type     = 0

[LOG] print_info: rope type        = 0

[LOG] print_info: rope scaling     = linear

[LOG] print_info: freq_base_train  = 100000.0

[LOG] print_info: freq_scale_train = 0.25

[LOG] print_info: n_ctx_orig_yarn  = 16384

[LOG] print_info: rope_finetuned   = unknown

[LOG] print_info: model type       = ?B

[LOG] print_info: model params     = 1.35 B

[LOG] print_info: general.name     = Deepseek Coder 1.3b Instruct

[LOG] print_info: vocab type       = BPE

[LOG] print_info: n_vocab          = 32256

[LOG] print_info: n_merges         = 31757

[LOG] print_info: BOS token        = 32013 '<ï½œbeginâ–ofâ–sentenceï½œ>'

[LOG] print_info: EOS token        = 32021 '<|EOT|>'

[LOG] print_info: EOT token        = 32014 '<ï½œendâ–ofâ–sentenceï½œ>'

[LOG] print_info: PAD token        = 32014 '<ï½œendâ–ofâ–sentenceï½œ>'

[LOG] print_info: LF token         = 185 'ÄŠ'

[LOG] print_info: FIM PRE token    = 32016 '<ï½œfimâ–beginï½œ>'

[LOG] print_info: FIM SUF token    = 32015 '<ï½œfimâ–holeï½œ>'

[LOG] print_info: FIM MID token    = 32017 '<ï½œfimâ–endï½œ>'

[LOG] print_info: EOG token        = 32014 '<ï½œendâ–ofâ–sentenceï½œ>'

[LOG] print_info: EOG token        = 32021 '<|EOT|>'

[LOG] print_info: max token length = 128

[LOG] load_tensors: loading model tensors, this can take a while... (mmap = true)

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] load_tensors: offloading 0 repeating layers to GPU

[LOG] load_tensors: offloaded 0/25 layers to GPU

[LOG] load_tensors:   CPU_REPACK model buffer size =   651.38 MiB

[LOG] load_tensors:   CPU_Mapped model buffer size =   738.88 MiB

[LOG] ...................srv  log_server_r: request: POST /completion 127.0.0.1 503

[LOG] ........................................................................

[LOG] llama_context: constructing llama_context

[LOG] llama_context: n_seq_max     = 1

[LOG] llama_context: n_ctx         = 1024

[LOG] llama_context: n_ctx_per_seq = 1024

[LOG] llama_context: n_batch       = 1024

[LOG] llama_context: n_ubatch      = 512

[LOG] llama_context: causal_attn   = 1

[LOG] llama_context: flash_attn    = 0

[LOG] llama_context: freq_base     = 100000.0

[LOG] llama_context: freq_scale    = 0.25

[LOG] llama_context: n_ctx_per_seq (1024) < n_ctx_train (16384) -- the full capacity of the model will not be utilized

[LOG] llama_context:        CPU  output buffer size =     0.12 MiB

[LOG] llama_kv_cache_unified:        CPU KV buffer size =   192.00 MiB

[LOG] llama_kv_cache_unified: size =  192.00 MiB (  1024 cells,  24 layers,  1 seqs), K (f16):   96.00 MiB, V (f16):   96.00 MiB

[LOG] llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility

[LOG] llama_context:        CPU compute buffer size =    67.00 MiB

[LOG] llama_context: graph nodes  = 870

[LOG] llama_context: graph splits = 1

[LOG] common_init_from_params: setting dry_penalty_last_n to ctx_size = 1024

[LOG] common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

[LOG] srv          init: initializing slots, n_slots = 1

[LOG] slot         init: id  0 | task -1 | new slot n_ctx_slot = 1024

[LOG] main: model loaded

[LOG] main: chat template, chat_template: {% if not add_generation_prompt is defined %}

[LOG] {% set add_generation_prompt = false %}

[LOG] {% endif %}

[LOG] {%- set ns = namespace(found=false) -%}

[LOG] {%- for message in messages -%}

[LOG]     {%- if message['role'] == 'system' -%}

[LOG]         {%- set ns.found = true -%}

[LOG]     {%- endif -%}

[LOG] {%- endfor -%}

[LOG] {{bos_token}}{%- if not ns.found -%}

[LOG] {{'You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n'}}

[LOG] {%- endif %}

[LOG] {%- for message in messages %}

[LOG]     {%- if message['role'] == 'system' %}

[LOG] {{ message['content'] }}

[LOG]     {%- else %}

[LOG]         {%- if message['role'] == 'user' %}

[LOG] {{'### Instruction:\n' + message['content'] + '\n'}}

[LOG]         {%- else %}

[LOG] {{'### Response:\n' + message['content'] + '\n<|EOT|>\n'}}

[LOG]         {%- endif %}

[LOG]     {%- endif %}

[LOG] {%- endfor %}

[LOG] {% if add_generation_prompt %}

[LOG] {{'### Response:'}}

[LOG] {% endif %}, example_format: 'You are a helpful assistant### Instruction:

[LOG] Hello

[LOG] ### Response:

[LOG] Hi there

[LOG] <|EOT|>

[LOG] ### Instruction:

[LOG] How are you?

[LOG] ### Response:

[LOG] '

[LOG] main: server is listening on http://127.0.0.1:5001 - starting the main loop

[LOG] srv  update_slots: all slots are idle

[LOG] slot launch_slot_: id  0 | task 0 | processing task

[LOG] slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 1024, n_keep = 0, n_prompt_tokens = 2

[LOG] slot update_slots: id  0 | task 0 | kv cache rm [0, end)

[LOG] slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 2, n_tokens = 2, progress = 1.000000

[LOG] slot update_slots: id  0 | task 0 | prompt done, n_past = 2, n_tokens = 2

[LOG] slot      release: id  0 | task 0 | stop processing: n_past = 5, truncated = 0

[LOG] slot print_timing: id  0 | task 0 | 

[LOG] prompt eval time =      48.89 ms /     2 tokens (   24.44 ms per token,    40.91 tokens per second)

[LOG]        eval time =      82.50 ms /     4 tokens (   20.62 ms per token,    48.49 tokens per second)

[LOG]       total time =     131.38 ms /     6 tokens

[LOG] srv  update_slots: all slots are idle

[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 200


ðŸ§‘ You: hello, how are you?
ðŸ¤– AI: I'm here to help with your coding needs or any other questions related to programming in general. Let me know what else can be done for you today..   [/code] ]}<br />]]}}"

```jsonhint-javascript {[{ "type": "/", "-106, -92)", 384]: ["{{#if (eq 'a' \"b\") }}{{'Hello World'} {{^is_admin}}{{.user.name... (ringkas)

[ðŸŒ¾] Auto-muat model dalam RAKYAT MODE...
[LOG] load_backend: loaded RPC backend from C:\Users\Reza Pahlevi\Downloads\AI-Tools\GUI\textgen gui\llama-b5899-bin-win-cpu-x64\ggml-rpc.dll
[LOG] load_backend: loaded CPU backend from C:\Users\Reza Pahlevi\Downloads\AI-Tools\GUI\textgen gui\llama-b5899-bin-win-cpu-x64\ggml-cpu-haswell.dll
[LOG] build: 5899 (68e37a61) with clang version 19.1.5 for x86_64-pc-windows-msvc
[LOG] system info: n_threads = 6, n_threads_batch = 6, total_threads = 6
[LOG] 
[LOG] system_info: n_threads = 6 (n_threads_batch = 6) / 6 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
[LOG] 
[LOG] main: binding port with default address family
[LOG] main: HTTP server is listening, hostname: 127.0.0.1, port: 5001, http threads: 5
[LOG] main: loading model
[LOG] srv    load_model: loading model 'D:/Program Files/AI-Tools/Models/deepseek-coder-1.3b-instruct.Q4_0.gguf'
[LOG] llama_model_loader: loaded meta data with 34 key-value pairs and 219 tensors from D:/Program Files/AI-Tools/Models/deepseek-coder-1.3b-instruct.Q4_0.gguf (version GGUF V3 (latest))
[LOG] llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[LOG] llama_model_loader: - kv   0:                       general.architecture str              = llama
[LOG] llama_model_loader: - kv   1:                               general.type str              = model
[LOG] llama_model_loader: - kv   2:                               general.name str              = Deepseek Coder 1.3b Instruct
[LOG] llama_model_loader: - kv   3:                           general.finetune str              = instruct
[LOG] llama_model_loader: - kv   4:                           general.basename str              = deepseek-coder
[LOG] llama_model_loader: - kv   5:                         general.size_label str              = 1.3B
[LOG] llama_model_loader: - kv   6:                            general.license str              = other
[LOG] llama_model_loader: - kv   7:                       general.license.name str              = deepseek
[LOG] llama_model_loader: - kv   8:                       general.license.link str              = LICENSE
[LOG] llama_model_loader: - kv   9:                          llama.block_count u32              = 24
[LOG] llama_model_loader: - kv  10:                       llama.context_length u32              = 16384
[LOG] llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048
[LOG] llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 5504
[LOG] llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 16
[LOG] llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 16
[LOG] llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000
[LOG] llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
[LOG] llama_model_loader: - kv  17:                          general.file_type u32              = 2
[LOG] llama_model_loader: - kv  18:                           llama.vocab_size u32              = 32256
[LOG] llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
[LOG] llama_model_loader: - kv  20:                    llama.rope.scaling.type str              = linear
[LOG] llama_model_loader: - kv  21:                  llama.rope.scaling.factor f32              = 4.000000
[LOG] llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
[LOG] llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = deepseek-coder
[LOG] llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,32256]   = ["!", "\"", "#", "$", "%", "&", "'", ...
[LOG] llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,32256]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
[LOG] llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,31757]   = ["Ä  Ä ", "Ä  t", "Ä  a", "i n", "h e...
[LOG] llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 32013
[LOG] llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 32021
[LOG] llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 32014
[LOG] llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true
[LOG] llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false
[LOG] llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
[LOG] llama_model_loader: - kv  33:               general.quantization_version u32              = 2
[LOG] llama_model_loader: - type  f32:   49 tensors
[LOG] llama_model_loader: - type q4_0:  169 tensors
[LOG] llama_model_loader: - type q6_K:    1 tensors
[LOG] print_info: file format = GGUF V3 (latest)
[LOG] print_info: file type   = Q4_0
[LOG] print_info: file size   = 738.88 MiB (4.60 BPW) 
[LOG] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
[LOG] load: special_eot_id is not in special_eog_ids - the tokenizer config may be incorrect
[LOG] load: special tokens cache size = 22
[LOG] load: token to piece cache size = 0.1765 MB
[LOG] print_info: arch             = llama
[LOG] print_info: vocab_only       = 0
[LOG] print_info: n_ctx_train      = 16384
[LOG] print_info: n_embd           = 2048
[LOG] print_info: n_layer          = 24
[LOG] print_info: n_head           = 16
[LOG] print_info: n_head_kv        = 16
[LOG] print_info: n_rot            = 128
[LOG] print_info: n_swa            = 0
[LOG] print_info: is_swa_any       = 0
[LOG] print_info: n_embd_head_k    = 128
[LOG] print_info: n_embd_head_v    = 128
[LOG] print_info: n_gqa            = 1
[LOG] print_info: n_embd_k_gqa     = 2048
[LOG] print_info: n_embd_v_gqa     = 2048
[LOG] print_info: f_norm_eps       = 0.0e+00
[LOG] print_info: f_norm_rms_eps   = 1.0e-06
[LOG] print_info: f_clamp_kqv      = 0.0e+00
[LOG] print_info: f_max_alibi_bias = 0.0e+00
[LOG] print_info: f_logit_scale    = 0.0e+00
[LOG] print_info: f_attn_scale     = 0.0e+00
[LOG] print_info: n_ff             = 5504
[LOG] print_info: n_expert         = 0
[LOG] print_info: n_expert_used    = 0
[LOG] print_info: causal attn      = 1
[LOG] print_info: pooling type     = 0
[LOG] print_info: rope type        = 0
[LOG] print_info: rope scaling     = linear
[LOG] print_info: freq_base_train  = 100000.0
[LOG] print_info: freq_scale_train = 0.25
[LOG] print_info: n_ctx_orig_yarn  = 16384
[LOG] print_info: rope_finetuned   = unknown
[LOG] print_info: model type       = ?B
[LOG] print_info: model params     = 1.35 B
[LOG] print_info: general.name     = Deepseek Coder 1.3b Instruct
[LOG] print_info: vocab type       = BPE
[LOG] print_info: n_vocab          = 32256
[LOG] print_info: n_merges         = 31757
[LOG] print_info: BOS token        = 32013 '<ï½œbeginâ–ofâ–sentenceï½œ>'
[LOG] print_info: EOS token        = 32021 '<|EOT|>'
[LOG] print_info: EOT token        = 32014 '<ï½œendâ–ofâ–sentenceï½œ>'
[LOG] print_info: PAD token        = 32014 '<ï½œendâ–ofâ–sentenceï½œ>'
[LOG] print_info: LF token         = 185 'ÄŠ'
[LOG] print_info: FIM PRE token    = 32016 '<ï½œfimâ–beginï½œ>'
[LOG] print_info: FIM SUF token    = 32015 '<ï½œfimâ–holeï½œ>'
[LOG] print_info: FIM MID token    = 32017 '<ï½œfimâ–endï½œ>'
[LOG] print_info: EOG token        = 32014 '<ï½œendâ–ofâ–sentenceï½œ>'
[LOG] print_info: EOG token        = 32021 '<|EOT|>'
[LOG] print_info: max token length = 128
[LOG] load_tensors: loading model tensors, this can take a while... (mmap = true)
[LOG] load_tensors: offloading 0 repeating layers to GPU
[LOG] load_tensors: offloaded 0/25 layers to GPU
[LOG] load_tensors:   CPU_REPACK model buffer size =   651.38 MiB
[LOG] load_tensors:   CPU_Mapped model buffer size =   738.88 MiB
[LOG] ...........................................................................................
[LOG] llama_context: constructing llama_context
[LOG] llama_context: n_seq_max     = 1
[LOG] llama_context: n_ctx         = 1024
[LOG] llama_context: n_ctx_per_seq = 1024
[LOG] llama_context: n_batch       = 1024
[LOG] llama_context: n_ubatch      = 512
[LOG] llama_context: causal_attn   = 1
[LOG] llama_context: flash_attn    = 0
[LOG] llama_context: freq_base     = 100000.0
[LOG] llama_context: freq_scale    = 0.25
[LOG] llama_context: n_ctx_per_seq (1024) < n_ctx_train (16384) -- the full capacity of the model will not be utilized
[LOG] llama_context:        CPU  output buffer size =     0.12 MiB
[LOG] llama_kv_cache_unified:        CPU KV buffer size =   192.00 MiB
[LOG] llama_kv_cache_unified: size =  192.00 MiB (  1024 cells,  24 layers,  1 seqs), K (f16):   96.00 MiB, V (f16):   96.00 MiB
[LOG] llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility
[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503
[LOG] llama_context:        CPU compute buffer size =    67.00 MiB
[LOG] llama_context: graph nodes  = 870
[LOG] llama_context: graph splits = 1
[LOG] common_init_from_params: setting dry_penalty_last_n to ctx_size = 1024
[LOG] common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
[LOG] srv          init: initializing slots, n_slots = 1
[LOG] slot         init: id  0 | task -1 | new slot n_ctx_slot = 1024
[LOG] main: model loaded
[LOG] main: chat template, chat_template: {% if not add_generation_prompt is defined %}
[LOG] {% set add_generation_prompt = false %}
[LOG] {% endif %}
[LOG] {%- set ns = namespace(found=false) -%}
[LOG] {%- for message in messages -%}
[LOG]     {%- if message['role'] == 'system' -%}
[LOG]         {%- set ns.found = true -%}
[LOG]     {%- endif -%}
[LOG] {%- endfor -%}
[LOG] {{bos_token}}{%- if not ns.found -%}
[LOG] {{'You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n'}}
[LOG] {%- endif %}
[LOG] {%- for message in messages %}
[LOG]     {%- if message['role'] == 'system' %}
[LOG] {{ message['content'] }}
[LOG]     {%- else %}
[LOG]         {%- if message['role'] == 'user' %}
[LOG] {{'### Instruction:\n' + message['content'] + '\n'}}
[LOG]         {%- else %}
[LOG] {{'### Response:\n' + message['content'] + '\n<|EOT|>\n'}}
[LOG]         {%- endif %}
[LOG]     {%- endif %}
[LOG] {%- endfor %}
[LOG] {% if add_generation_prompt %}
[LOG] {{'### Response:'}}
[LOG] {% endif %}, example_format: 'You are a helpful assistant### Instruction:
[LOG] Hello
[LOG] ### Response:
[LOG] Hi there
[LOG] <|EOT|>
[LOG] ### Instruction:
[LOG] How are you?
[LOG] ### Response:
[LOG] '
[LOG] main: server is listening on http://127.0.0.1:5001 - starting the main loop
[LOG] srv  update_slots: all slots are idle
[LOG] slot launch_slot_: id  0 | task 0 | processing task
[LOG] slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 1024, n_keep = 0, n_prompt_tokens = 2
[LOG] slot update_slots: id  0 | task 0 | kv cache rm [0, end)
[LOG] slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 2, n_tokens = 2, progress = 1.000000
[LOG] slot update_slots: id  0 | task 0 | prompt done, n_past = 2, n_tokens = 2
[LOG] slot      release: id  0 | task 0 | stop processing: n_past = 6, truncated = 0
[LOG] slot print_timing: id  0 | task 0 | 
[LOG] prompt eval time =      43.70 ms /     2 tokens (   21.85 ms per token,    45.77 tokens per second)
[LOG]        eval time =     122.13 ms /     5 tokens (   24.43 ms per token,    40.94 tokens per second)
[LOG]       total time =     165.83 ms /     7 tokens
[LOG] srv  update_slots: all slots are idle
[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 200
[ðŸŒ¾] Menjalankan dalam RAKYAT MODE: ctx-size 1024, RAM hemat!
[ðŸ›‘] Proses lama dihentikan.
[LOG] load_backend: loaded RPC backend from C:\Users\Reza Pahlevi\Downloads\AI-Tools\GUI\textgen gui\llama-b5899-bin-win-cpu-x64\ggml-rpc.dll
[LOG] load_backend: loaded CPU backend from C:\Users\Reza Pahlevi\Downloads\AI-Tools\GUI\textgen gui\llama-b5899-bin-win-cpu-x64\ggml-cpu-haswell.dll
[LOG] build: 5899 (68e37a61) with clang version 19.1.5 for x86_64-pc-windows-msvc
[LOG] system info: n_threads = 6, n_threads_batch = 6, total_threads = 6
[LOG] 
[LOG] system_info: n_threads = 6 (n_threads_batch = 6) / 6 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
[LOG] 
[LOG] main: binding port with default address family
[LOG] main: HTTP server is listening, hostname: 127.0.0.1, port: 5001, http threads: 5
[LOG] main: loading model
[LOG] srv    load_model: loading model 'D:/Program Files/AI-Tools/Models/mistral-7b-instruct-v0.1-q4_k_m.gguf'
[LOG] llama_model_loader: loaded meta data with 38 key-value pairs and 291 tensors from D:/Program Files/AI-Tools/Models/mistral-7b-instruct-v0.1-q4_k_m.gguf (version GGUF V3 (latest))
[LOG] llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[LOG] llama_model_loader: - kv   0:                       general.architecture str              = llama
[LOG] llama_model_loader: - kv   1:                               general.type str              = model
[LOG] llama_model_loader: - kv   2:                               general.name str              = Mistral 7B Instruct v0.1
[LOG] llama_model_loader: - kv   3:                            general.version str              = v0.1
[LOG] llama_model_loader: - kv   4:                           general.finetune str              = Instruct
[LOG] llama_model_loader: - kv   5:                           general.basename str              = Mistral
[LOG] llama_model_loader: - kv   6:                         general.size_label str              = 7B
[LOG] llama_model_loader: - kv   7:                            general.license str              = apache-2.0
[LOG] llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
[LOG] llama_model_loader: - kv   9:                  general.base_model.0.name str              = Mistral 7B v0.1
[LOG] llama_model_loader: - kv  10:               general.base_model.0.version str              = v0.1
[LOG] llama_model_loader: - kv  11:          general.base_model.0.organization str              = Mistralai
[LOG] llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/mistralai/Mist...
[LOG] llama_model_loader: - kv  13:                               general.tags arr[str,2]       = ["finetuned", "text-generation"]
[LOG] llama_model_loader: - kv  14:                          llama.block_count u32              = 32
[LOG] llama_model_loader: - kv  15:                       llama.context_length u32              = 32768
[LOG] llama_model_loader: - kv  16:                     llama.embedding_length u32              = 4096
[LOG] llama_model_loader: - kv  17:                  llama.feed_forward_length u32              = 14336
[LOG] llama_model_loader: - kv  18:                 llama.attention.head_count u32              = 32
[LOG] llama_model_loader: - kv  19:              llama.attention.head_count_kv u32              = 8
[LOG] llama_model_loader: - kv  20:                       llama.rope.freq_base f32              = 10000.000000
[LOG] llama_model_loader: - kv  21:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
[LOG] llama_model_loader: - kv  22:                          general.file_type u32              = 15
[LOG] llama_model_loader: - kv  23:                           llama.vocab_size u32              = 32000
[LOG] llama_model_loader: - kv  24:                 llama.rope.dimension_count u32              = 128
[LOG] llama_model_loader: - kv  25:                       tokenizer.ggml.model str              = llama
[LOG] llama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = default
[LOG] llama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
[LOG] llama_model_loader: - kv  28:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...
[LOG] llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
[LOG] llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 1
[LOG] llama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 2
[LOG] llama_model_loader: - kv  32:            tokenizer.ggml.unknown_token_id u32              = 0
[LOG] llama_model_loader: - kv  33:               tokenizer.ggml.add_bos_token bool             = true
[LOG] llama_model_loader: - kv  34:               tokenizer.ggml.add_eos_token bool             = false
[LOG] llama_model_loader: - kv  35:                    tokenizer.chat_template str              = {%- if messages[0]['role'] == 'system...
[LOG] llama_model_loader: - kv  36:            tokenizer.ggml.add_space_prefix bool             = false
[LOG] llama_model_loader: - kv  37:               general.quantization_version u32              = 2
[LOG] llama_model_loader: - type  f32:   65 tensors
[LOG] llama_model_loader: - type q4_K:  193 tensors
[LOG] llama_model_loader: - type q6_K:   33 tensors
[LOG] print_info: file format = GGUF V3 (latest)
[LOG] print_info: file type   = Q4_K - Medium
[LOG] print_info: file size   = 4.07 GiB (4.83 BPW) 
[LOG] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
[LOG] load: special tokens cache size = 3
[LOG] load: token to piece cache size = 0.1637 MB
[LOG] print_info: arch             = llama
[LOG] print_info: vocab_only       = 0
[LOG] print_info: n_ctx_train      = 32768
[LOG] print_info: n_embd           = 4096
[LOG] print_info: n_layer          = 32
[LOG] print_info: n_head           = 32
[LOG] print_info: n_head_kv        = 8
[LOG] print_info: n_rot            = 128
[LOG] print_info: n_swa            = 0
[LOG] print_info: is_swa_any       = 0
[LOG] print_info: n_embd_head_k    = 128
[LOG] print_info: n_embd_head_v    = 128
[LOG] print_info: n_gqa            = 4
[LOG] print_info: n_embd_k_gqa     = 1024
[LOG] print_info: n_embd_v_gqa     = 1024
[LOG] print_info: f_norm_eps       = 0.0e+00
[LOG] print_info: f_norm_rms_eps   = 1.0e-05
[LOG] print_info: f_clamp_kqv      = 0.0e+00
[LOG] print_info: f_max_alibi_bias = 0.0e+00
[LOG] print_info: f_logit_scale    = 0.0e+00
[LOG] print_info: f_attn_scale     = 0.0e+00
[LOG] print_info: n_ff             = 14336
[LOG] print_info: n_expert         = 0
[LOG] print_info: n_expert_used    = 0
[LOG] print_info: causal attn      = 1
[LOG] print_info: pooling type     = 0
[LOG] print_info: rope type        = 0
[LOG] print_info: rope scaling     = linear
[LOG] print_info: freq_base_train  = 10000.0
[LOG] print_info: freq_scale_train = 1
[LOG] print_info: n_ctx_orig_yarn  = 32768
[LOG] print_info: rope_finetuned   = unknown
[LOG] print_info: model type       = 7B
[LOG] print_info: model params     = 7.24 B
[LOG] print_info: general.name     = Mistral 7B Instruct v0.1
[LOG] print_info: vocab type       = SPM
[LOG] print_info: n_vocab          = 32000
[LOG] print_info: n_merges         = 0
[LOG] print_info: BOS token        = 1 '<s>'
[LOG] print_info: EOS token        = 2 '</s>'
[LOG] print_info: UNK token        = 0 '<unk>'
[LOG] print_info: LF token         = 13 '<0x0A>'
[LOG] print_info: EOG token        = 2 '</s>'
[LOG] print_info: max token length = 48
[LOG] load_tensors: loading model tensors, this can take a while... (mmap = true)
[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503
[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503
[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503
[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503
[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503
[LOG] srv  log_server_r: request:    400
[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503
[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503
[LOG] srv  log_server_r: request:    400
[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503
[LOG] load_tensors: offloading 0 repeating layers to GPU
[LOG] load_tensors: offloaded 0/33 layers to GPU
[LOG] load_tensors:   CPU_REPACK model buffer size =  3204.00 MiB
[LOG] load_tensors:   CPU_Mapped model buffer size =  4133.87 MiB
[LOG] .....................srv  log_server_r: request: POST /completion 127.0.0.1 503
[LOG] .................................srv  log_server_r: request: POST /completion 127.0.0.1 503
[LOG] srv  log_server_r: request:    400
[LOG] .........................................
[LOG] llama_context: constructing llama_context
[LOG] llama_context: n_seq_max     = 1
[LOG] llama_context: n_ctx         = 1024
[LOG] llama_context: n_ctx_per_seq = 1024
[LOG] llama_context: n_batch       = 1024
[LOG] llama_context: n_ubatch      = 512
[LOG] llama_context: causal_attn   = 1
[LOG] llama_context: flash_attn    = 0
[LOG] llama_context: freq_base     = 10000.0
[LOG] llama_context: freq_scale    = 1
[LOG] llama_context: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
[LOG] llama_context:        CPU  output buffer size =     0.12 MiB
[LOG] llama_kv_cache_unified:        CPU KV buffer size =   128.00 MiB
[LOG] llama_kv_cache_unified: size =  128.00 MiB (  1024 cells,  32 layers,  1 seqs), K (f16):   64.00 MiB, V (f16):   64.00 MiB
[LOG] llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility
[LOG] llama_context:        CPU compute buffer size =   110.01 MiB
[LOG] llama_context: graph nodes  = 1158
[LOG] llama_context: graph splits = 1
[LOG] common_init_from_params: setting dry_penalty_last_n to ctx_size = 1024
[LOG] common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 503
[LOG] srv          init: initializing slots, n_slots = 1
[LOG] slot         init: id  0 | task -1 | new slot n_ctx_slot = 1024
[LOG] main: model loaded
[LOG] main: chat template, chat_template: {%- if messages[0]['role'] == 'system' %}
[LOG]     {%- set system_message = messages[0]['content'] %}
[LOG]     {%- set loop_messages = messages[1:] %}
[LOG] {%- else %}
[LOG]     {%- set loop_messages = messages %}
[LOG] {%- endif %}
[LOG] 
[LOG] {{- bos_token }}
[LOG] {%- for message in loop_messages %}
[LOG]     {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}
[LOG]         {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}
[LOG]     {%- endif %}
[LOG]     {%- if message['role'] == 'user' %}
[LOG]         {%- if loop.first and system_message is defined %}
[LOG]             {{- ' [INST] ' + system_message + '\n\n' + message['content'] + ' [/INST]' }}
[LOG]         {%- else %}
[LOG]             {{- ' [INST] ' + message['content'] + ' [/INST]' }}
[LOG]         {%- endif %}
[LOG]     {%- elif message['role'] == 'assistant' %}
[LOG]         {{- ' ' + message['content'] + eos_token}}
[LOG]     {%- else %}
[LOG]         {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}
[LOG]     {%- endif %}
[LOG] {%- endfor %}
[LOG] , example_format: ' [INST] You are a helpful assistant
[LOG] 
[LOG] Hello [/INST] Hi there</s> [INST] How are you? [/INST]'
[LOG] main: server is listening on http://127.0.0.1:5001 - starting the main loop
[LOG] srv  update_slots: all slots are idle
[LOG] slot launch_slot_: id  0 | task 0 | processing task
[LOG] slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 1024, n_keep = 0, n_prompt_tokens = 2
[LOG] slot update_slots: id  0 | task 0 | kv cache rm [0, end)
[LOG] slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 2, n_tokens = 2, progress = 1.000000
[LOG] slot update_slots: id  0 | task 0 | prompt done, n_past = 2, n_tokens = 2
[LOG] slot      release: id  0 | task 0 | stop processing: n_past = 6, truncated = 0
[LOG] slot print_timing: id  0 | task 0 | 
[LOG] prompt eval time =     319.99 ms /     2 tokens (  160.00 ms per token,     6.25 tokens per second)
[LOG]        eval time =     831.43 ms /     5 tokens (  166.29 ms per token,     6.01 tokens per second)
[LOG]       total time =    1151.43 ms /     7 tokens
[LOG] srv  update_slots: all slots are idle
[LOG] srv  log_server_r: request: POST /completion 127.0.0.1 200

ðŸ§‘ You: how are you doing today?
ðŸ¤– AI: i am doing well. just here to help you with any questions or concerns you may have!

